{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, create a conda environment with scgpt and other deps installed.\n",
    "\n",
    "```bash\n",
    "conda create -y -n scgpt python=3.11\n",
    "conda activate scgpt\n",
    "# pip install -r requirements.txt\n",
    "pip install scgpt\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('../../data/replogle22/replogle22_processed.h5ad'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/replogle22/replogle22_processed.h5ad ../../data/replogle22/replogle22_processed.h5ad')\n",
    "\n",
    "if not os.path.exists('../../data/replogle22/replogle22_names_df_vsrest.pkl'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/replogle22/replogle22_names_df_vsrest.pkl ../../data/replogle22/replogle22_names_df_vsrest.pkl')\n",
    "\n",
    "if not os.path.exists('../../data/replogle22/replogle22_scores_df_vsrest.pkl'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/replogle22/replogle22_scores_df_vsrest.pkl ../../data/replogle22/replogle22_scores_df_vsrest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the numpy files\n",
    "try:\n",
    "    names_df_vsrest = np.load('../../data/replogle22/replogle22_names_df_vsrest.pkl', allow_pickle=True)\n",
    "    print(\"Successfully loaded names_df_vsrest\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading names_df_vsrest: {e}\")\n",
    "\n",
    "try:\n",
    "    scores_df_vsrest = np.load('../../data/replogle22/replogle22_scores_df_vsrest.pkl', allow_pickle=True)\n",
    "    print(\"Successfully loaded scores_df_vsrest\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading scores_df_vsrest: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.stats import ranksums # Added ranksums\n",
    "import scienceplots\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # For finding the 'analyses' package\n",
    "from common import *\n",
    "\n",
    "\n",
    "DATASET_NAME = 'replogle22'\n",
    "\n",
    "# Initialize analysis using the common function\n",
    "(\n",
    "    adata,\n",
    "    pert_means, # This is the dictionary from get_pert_means(adata) \n",
    "    total_mean_original,\n",
    "    ctrl_mean_original,\n",
    "    DATASET_NAME,\n",
    "    DATASET_CELL_COUNTS,\n",
    "    DATASET_PERTS_TO_SWEEP,\n",
    "    dataset_specific_subdir, # e.g. \"norman19\" or \"replogle22\"\n",
    "    DATA_CACHE_DIR, # Base cache dir, e.g., \"../../../data/\"\n",
    "    original_np_random_state,\n",
    "    ANALYSIS_DIR,\n",
    "    pert_normalized_abs_scores_vsrest,\n",
    "    pert_counts,\n",
    "    scores_df_vsrest,\n",
    "    names_df_vsrest,\n",
    ") = initialize_analysis(DATASET_NAME, 'modeling_with_gears')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_means.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================GEARS DATA PREPARATION ===================\n",
    "\n",
    "# Import required libraries\n",
    "from gears import PertData, GEARS\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists('../../data/gears_predictions.pkl'):\n",
    "\n",
    "    print(\"Starting GEARS data preparation...\")\n",
    "\n",
    "    # Create a copy of the original data and subsample cells\n",
    "    print(\"Creating data copy...\")\n",
    "    adata_gears = adata.copy()\n",
    "    np.random.seed(42)  # Set random seed for reproducibility\n",
    "\n",
    "    adata_gears = adata_gears.copy()\n",
    "\n",
    "    # Process condition labels\n",
    "    print(\"Processing condition labels...\")\n",
    "    adata_gears.obs['condition'] = adata_gears.obs['condition'].astype(str) + '+ctrl'\n",
    "    adata_gears.obs['condition'] = adata_gears.obs['condition'].str.replace('control+ctrl', 'ctrl')\n",
    "\n",
    "    # Get unique perturbations\n",
    "    print(\"Getting unique perturbations...\")\n",
    "    all_perturbations = adata_gears.obs['condition'].unique()\n",
    "    all_perturbations = all_perturbations[all_perturbations != 'ctrl']\n",
    "    print(f\"Found {len(all_perturbations)} unique perturbations\")\n",
    "\n",
    "    # Split perturbations into first and second half\n",
    "    print(\"Splitting perturbations into halves...\")\n",
    "    first_half_perturbations = all_perturbations[:len(all_perturbations)//2]\n",
    "    second_half_perturbations = all_perturbations[len(all_perturbations)//2:]\n",
    "    print(f\"First half: {len(first_half_perturbations)} perturbations\")\n",
    "    print(f\"Second half: {len(second_half_perturbations)} perturbations\")\n",
    "\n",
    "    # Process first half splits\n",
    "    print(\"\\nProcessing first half splits...\")\n",
    "    first_half_train_val = first_half_perturbations\n",
    "    np.random.shuffle(first_half_train_val)\n",
    "    split_idx = int(len(first_half_train_val) * 0.9) # 90% train, 10% val\n",
    "    first_half_train = first_half_train_val[:split_idx]\n",
    "    first_half_val = first_half_train_val[split_idx:]\n",
    "\n",
    "    first_half_split_dict = {\n",
    "        'train': first_half_train,\n",
    "        'val': first_half_val,\n",
    "        'test': first_half_perturbations  # All first half perturbations as test\n",
    "    }\n",
    "    print(f\"First half splits - Train: {len(first_half_train)}, Val: {len(first_half_val)}\")\n",
    "\n",
    "    # Process second half splits\n",
    "    print(\"\\nProcessing second half splits...\")\n",
    "    second_half_train_val = second_half_perturbations\n",
    "    np.random.shuffle(second_half_train_val) \n",
    "    split_idx = int(len(second_half_train_val) * 0.9)  # 90% train, 10% val\n",
    "    second_half_train = second_half_train_val[:split_idx]\n",
    "    second_half_val = second_half_train_val[split_idx:]\n",
    "\n",
    "    second_half_split_dict = {\n",
    "        'train': second_half_train,\n",
    "        'val': second_half_val,\n",
    "        'test': second_half_perturbations  # All second half perturbations as test\n",
    "    }\n",
    "    print(f\"Second half splits - Train: {len(second_half_train)}, Val: {len(second_half_val)}\")\n",
    "\n",
    "    # Prepare data for GEARS\n",
    "    print(\"\\nPreparing data for GEARS...\")\n",
    "    # Add control to perturbation lists\n",
    "    first_half_perturbations = np.concatenate([['ctrl'], first_half_perturbations])\n",
    "    second_half_perturbations = np.concatenate([['ctrl'], second_half_perturbations])\n",
    "\n",
    "    # Create subsetted datasets\n",
    "    print(\"Creating subsetted datasets...\")\n",
    "    adata_first_half_gears = adata_gears[adata_gears.obs['condition'].isin(first_half_perturbations)].copy()\n",
    "    adata_second_half_gears = adata_gears[adata_gears.obs['condition'].isin(second_half_perturbations)].copy()\n",
    "    print(f\"First half dataset size: {adata_first_half_gears.n_obs} cells\")\n",
    "    print(f\"Second half dataset size: {adata_second_half_gears.n_obs} cells\")\n",
    "\n",
    "    # Process first half data\n",
    "    print(\"\\nProcessing first half data with GEARS...\")\n",
    "    pert_data_first_half = PertData('../../data')\n",
    "    pert_data_first_half.new_data_process(dataset_name='replogle22_gears_first_half_gears', adata=adata_first_half_gears)\n",
    "    # Filter out genes not in gene2go from each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        original_count = len(first_half_split_dict[split])\n",
    "        first_half_split_dict[split] = [gene for gene in first_half_split_dict[split] \n",
    "                                    if gene.replace('+ctrl', '') in pert_data_first_half.gene2go.keys()]\n",
    "        filtered_count = len(first_half_split_dict[split])\n",
    "        print(f\"{split} split: {filtered_count}/{original_count} genes kept ({original_count - filtered_count} removed)\")\n",
    "    with open('../../data/replogle22_gears_first_half_split_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(first_half_split_dict, f)\n",
    "    pert_data_first_half.load(data_path='../../data/replogle22_gears_first_half_gears')\n",
    "    pert_data_first_half.prepare_split(split='custom', seed=42, split_dict_path='../../data/replogle22_gears_first_half_split_dict.pkl')\n",
    "    print(\"First half data processing complete\")\n",
    "\n",
    "    # Process second half data\n",
    "    print(\"\\nProcessing second half data with GEARS...\")\n",
    "    pert_data_second_half = PertData('../../data')\n",
    "    pert_data_second_half.new_data_process(dataset_name='replogle22_gears_second_half_gears', adata=adata_second_half_gears)\n",
    "    # Filter out genes not in gene2go from each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        original_count = len(second_half_split_dict[split])\n",
    "        second_half_split_dict[split] = [gene for gene in second_half_split_dict[split] \n",
    "                                    if gene.replace('+ctrl', '') in pert_data_second_half.gene2go.keys()]\n",
    "        filtered_count = len(second_half_split_dict[split])\n",
    "        print(f\"{split} split: {filtered_count}/{original_count} genes kept ({original_count - filtered_count} removed)\")\n",
    "    with open('../../data/replogle22_gears_second_half_split_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(second_half_split_dict, f)\n",
    "    pert_data_second_half.load(data_path='../../data/replogle22_gears_second_half_gears')\n",
    "    pert_data_second_half.prepare_split(split='custom', seed=42, split_dict_path='../../data/replogle22_gears_second_half_split_dict.pkl')\n",
    "    print(\"Second half data processing complete\")\n",
    "\n",
    "    print(\"\\nGEARS data preparation completed successfully!\")\n",
    "\n",
    "\n",
    "    # ===================GEARS MODEL TRAINING ===================\n",
    "\n",
    "    pert_data_first_half.get_dataloader(batch_size = 32, test_batch_size = 512)\n",
    "    pert_data_second_half.get_dataloader(batch_size = 32, test_batch_size = 512)\n",
    "\n",
    "    gears_model_first_half = GEARS(pert_data_first_half, device = 'cuda', \n",
    "                            weight_bias_track = False, \n",
    "                            proj_name = 'first_half', \n",
    "                            exp_name = 'first_half')\n",
    "    gears_model_first_half.model_initialize()\n",
    "\n",
    "    gears_model_second_half = GEARS(pert_data_second_half, device = 'cuda', \n",
    "                            weight_bias_track = False, \n",
    "                            proj_name = 'second_half', \n",
    "                            exp_name = 'second_half')\n",
    "    gears_model_second_half.model_initialize()\n",
    "\n",
    "    gears_model_first_half.train(epochs = 10)\n",
    "    gears_model_second_half.train(epochs = 10)\n",
    "\n",
    "    os.makedirs(\"../../data/gears_models\", exist_ok=True)\n",
    "\n",
    "    gears_model_first_half.save_model('../../data/gears_models/first_half')\n",
    "    gears_model_second_half.save_model('../../data/gears_models/second_half')\n",
    "\n",
    "\n",
    "\n",
    "    # ===================GEARS PREDICTIONS ===================\n",
    "\n",
    "    # Compile GEARS predictions\n",
    "    first_half_perturbations = [gene for gene in first_half_perturbations if gene.replace('+ctrl', '') in pert_data_first_half.gene2go.keys()]\n",
    "    second_half_perturbations = [gene for gene in second_half_perturbations if gene.replace('+ctrl', '') in pert_data_second_half.gene2go.keys()]\n",
    "\n",
    "    first_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in first_half_perturbations if gene != 'ctrl']\n",
    "    second_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in second_half_perturbations if gene != 'ctrl']\n",
    "\n",
    "    predictions_first_half = gears_model_second_half.predict(first_half_perturbations_without_ctrl)\n",
    "    predictions_second_half = gears_model_first_half.predict(second_half_perturbations_without_ctrl)\n",
    "\n",
    "    # Combine predictions from both halves\n",
    "    gears_predictions = {}\n",
    "    for pert in tqdm(predictions_first_half.keys()):\n",
    "        gears_predictions[pert] = predictions_first_half[pert]\n",
    "    for pert in tqdm(predictions_second_half.keys()):\n",
    "        gears_predictions[pert] = predictions_second_half[pert]\n",
    "    gears_predictions['control'] = pert_means['control']\n",
    "\n",
    "    # Save GEARS predictions to pickle file\n",
    "    with open('../../data/gears_predictions.pkl', 'wb') as f:\n",
    "        pickle.dump(gears_predictions, f)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load GEARS predictions from pickle file\n",
    "    with open('../../data/gears_predictions.pkl', 'rb') as f:\n",
    "        gears_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================SCGPT DATA PREPARATION ===================\n",
    "\n",
    "# Import required libraries\n",
    "from gears import PertData, GEARS\n",
    "import pickle\n",
    "\n",
    "print(\"Starting scGPT data preparation...\")\n",
    "\n",
    "# Create a copy of the original data and subsample cells\n",
    "print(\"Creating data copy...\")\n",
    "adata_gears = adata.copy()\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "\n",
    "# Create a new AnnData object with zeros for all genes\n",
    "print(\"Creating zero-filled AnnData object...\")\n",
    "perturbed_genes = list(adata_gears.obs['condition'].unique())\n",
    "perturbed_genes.remove('control')\n",
    "missing_perturbed_genes = [gene for gene in perturbed_genes if gene not in adata_gears.var.gene_name.tolist()]\n",
    "zero_adata = sc.AnnData(\n",
    "    X=np.zeros((adata_gears.n_obs, len(missing_perturbed_genes))),\n",
    "    obs=adata_gears.obs.copy(),\n",
    "    var=pd.DataFrame(index=missing_perturbed_genes)\n",
    ")\n",
    "zero_adata.var['gene_name'] = missing_perturbed_genes\n",
    "\n",
    "# Concatenate the original and zero-filled data along the genes axis\n",
    "print(\"Concatenating datasets...\")\n",
    "adata_gears = sc.concat([adata_gears, zero_adata], axis=1, join='outer', merge='first')\n",
    "\n",
    "print(f\"Final dataset size: {adata_gears.n_obs} cells, {adata_gears.n_vars} genes\")\n",
    "\n",
    "# Process condition labels\n",
    "print(\"Processing condition labels...\")\n",
    "adata_gears.obs['condition'] = adata_gears.obs['condition'].astype(str) + '+ctrl'\n",
    "adata_gears.obs['condition'] = adata_gears.obs['condition'].str.replace('control+ctrl', 'ctrl')\n",
    "\n",
    "# Get unique perturbations\n",
    "print(\"Getting unique perturbations...\")\n",
    "all_perturbations = adata_gears.obs['condition'].unique()\n",
    "all_perturbations = all_perturbations[all_perturbations != 'ctrl']\n",
    "print(f\"Found {len(all_perturbations)} unique perturbations\")\n",
    "\n",
    "# Split perturbations into first and second half\n",
    "print(\"Splitting perturbations into halves...\")\n",
    "first_half_perturbations = all_perturbations[:len(all_perturbations)//2]\n",
    "second_half_perturbations = all_perturbations[len(all_perturbations)//2:]\n",
    "\n",
    "# Process first half splits\n",
    "print(\"\\nProcessing first half splits...\")\n",
    "first_half_train_val = first_half_perturbations\n",
    "np.random.shuffle(first_half_train_val)\n",
    "split_idx = int(len(first_half_train_val) * 0.9) # 90% train, 10% val\n",
    "first_half_train = first_half_train_val[:split_idx]\n",
    "first_half_val = first_half_train_val[split_idx:]\n",
    "\n",
    "# Prepare data for scGPT\n",
    "print(\"\\nPreparing data for scGPT...\")\n",
    "# Load gene2go mapping file\n",
    "gene2go_path = \"../../data/gene2go_all.pkl\"\n",
    "with open(gene2go_path, 'rb') as f:\n",
    "    gene2go = pickle.load(f)\n",
    "first_half_perturbations = [pert for pert in first_half_perturbations if pert.replace('+ctrl','') in list(gene2go.keys())]\n",
    "second_half_perturbations = [pert for pert in second_half_perturbations if pert.replace('+ctrl','') in list(gene2go.keys())]\n",
    "# Add control to perturbation lists\n",
    "first_half_perturbations = np.concatenate([['ctrl'], first_half_perturbations])\n",
    "second_half_perturbations = np.concatenate([['ctrl'], second_half_perturbations])\n",
    "\n",
    "print(f\"First half: {len(first_half_perturbations)} perturbations\")\n",
    "print(f\"Second half: {len(second_half_perturbations)} perturbations\")\n",
    "\n",
    "# Create subsetted datasets\n",
    "print(\"Creating subsetted datasets...\")\n",
    "adata_first_half_gears = adata_gears[adata_gears.obs['condition'].isin(first_half_perturbations)].copy()\n",
    "adata_second_half_gears = adata_gears[adata_gears.obs['condition'].isin(second_half_perturbations)].copy()\n",
    "print(f\"First half dataset size: {adata_first_half_gears.n_obs} cells\")\n",
    "print(f\"Second half dataset size: {adata_second_half_gears.n_obs} cells\")\n",
    "\n",
    "# Process first half data\n",
    "print(\"\\nProcessing first half data with scGPT...\")\n",
    "pert_data_first_half = PertData('../../data')\n",
    "pert_data_first_half.new_data_process(dataset_name='replogle22_gears_first_half_scgpt', adata=adata_first_half_gears)\n",
    "pert_data_first_half.load(data_path='../../data/replogle22_gears_first_half_scgpt')\n",
    "pert_data_first_half.prepare_split(split='simulation', seed=42, train_gene_set_size = 0.9)\n",
    "print(\"First half data processing complete\")\n",
    "\n",
    "# Process second half data\n",
    "print(\"\\nProcessing second half data with scGPT...\")\n",
    "pert_data_second_half = PertData('../../data')\n",
    "pert_data_second_half.new_data_process(dataset_name='replogle22_gears_second_half_scgpt', adata=adata_second_half_gears)\n",
    "pert_data_second_half.load(data_path='../../data/replogle22_gears_second_half_scgpt')\n",
    "pert_data_second_half.prepare_split(split='simulation', seed=42, train_gene_set_size = 0.9)\n",
    "print(\"Second half data processing complete\")\n",
    "\n",
    "pert_data_first_half.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "pert_data_second_half.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "\n",
    "print(\"\\scGPT data preparation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================SCGPT DEPENDENCIES LOAD ===================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gears import PertData, GEARS\n",
    "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_relative_error,\n",
    ")\n",
    "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import set_seed, map_raw_id_to_vocab_id, compute_perturbation_metrics\n",
    "\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scgpt.utils import load_pretrained\n",
    "\n",
    "import gdown\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model_dir = \"../../data/scGPT_CP\"\n",
    "if not os.path.exists(model_dir):\n",
    "    !mkdir -p $model_dir\n",
    "    gdown.download_folder(\n",
    "        \"https://drive.google.com/drive/folders/1_GROJTzXiAV8HB4imruOTk6PEGuNOcgB?usp=sharing\",\n",
    "        output=model_dir,\n",
    "    )\n",
    "\n",
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 0\n",
    "include_zero_gene = \"all\"\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "amp = True\n",
    "load_model = \"../../data/scGPT_CP\"\n",
    "load_param_prefixs = [\n",
    "    \"encoder\",\n",
    "    \"value_encoder\",\n",
    "    \"transformer_encoder\",\n",
    "]\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4  # or 1e-4\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "epochs = 10\n",
    "schedule_interval = 1\n",
    "early_stop = 5\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "n_layers_cls = 3\n",
    "dropout = 0  # dropout probability\n",
    "use_fast_transformer = True  # whether to use fast transformer\n",
    "\n",
    "# logging\n",
    "log_interval = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_name = 'reploggle22'\n",
    "\n",
    "save_dir = Path(f\"../../data/save/dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"saving to {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "# log running date and current git commit\n",
    "logger.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "model_dir = Path(load_model)\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "pert_data_first_half.adata.var[\"id_in_vocab\"] = [\n",
    "    1 if gene in vocab else -1 for gene in pert_data_first_half.adata.var[\"gene_name\"]\n",
    "]\n",
    "gene_ids_in_vocab = np.array(pert_data_first_half.adata.var[\"id_in_vocab\"])\n",
    "logger.info(\n",
    "    f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "    f\"in vocabulary of size {len(vocab)}.\"\n",
    ")\n",
    "genes = pert_data_first_half.adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "# model\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "logger.info(\n",
    "    f\"Resume model from {model_file}, the model args will override the \"\n",
    "    f\"config {model_config_file}.\"\n",
    ")\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)\n",
    "\n",
    "pretrained_dict = torch.load(model_file)\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model_first_half = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "model_first_half = load_pretrained(model_first_half, pretrained_dict)\n",
    "model_first_half = model_first_half.to(device)\n",
    "\n",
    "# Create a copy for second half\n",
    "model_second_half = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "model_second_half.load_state_dict(model_first_half.state_dict())\n",
    "model_second_half = model_second_half.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================SCGPT FINETUNING ===================\n",
    "\n",
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# First half model optimizers and schedulers\n",
    "optimizer_first = torch.optim.Adam(model_first_half.parameters(), lr=lr)\n",
    "scheduler_first = torch.optim.lr_scheduler.StepLR(optimizer_first, schedule_interval, gamma=0.9)\n",
    "scaler_first = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "# Second half model optimizers and schedulers  \n",
    "optimizer_second = torch.optim.Adam(model_second_half.parameters(), lr=lr)\n",
    "scheduler_second = torch.optim.lr_scheduler.StepLR(optimizer_second, schedule_interval, gamma=0.9)\n",
    "scaler_second = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "\n",
    "# First half model training\n",
    "best_val_loss_first = float(\"inf\")\n",
    "best_val_corr_first = 0\n",
    "best_model_first = None\n",
    "patience_first = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loader = pert_data_first_half.dataloader[\"train_loader\"]\n",
    "    valid_loader = pert_data_first_half.dataloader[\"val_loader\"]\n",
    "\n",
    "    train(\n",
    "        model_first_half,\n",
    "        train_loader,\n",
    "        epoch,\n",
    "        device,\n",
    "        criterion,\n",
    "        optimizer_first,\n",
    "        scheduler_first,\n",
    "        scaler_first,\n",
    "        n_genes,\n",
    "        gene_ids,\n",
    "        include_zero_gene,\n",
    "        max_seq_len,\n",
    "        CLS,\n",
    "        CCE,\n",
    "        MVC,\n",
    "        ECS,\n",
    "        log_interval,\n",
    "        logger\n",
    "    )\n",
    "\n",
    "    val_res = eval_perturb(valid_loader, model_first_half, device, include_zero_gene, gene_ids)\n",
    "    val_metrics = compute_perturbation_metrics(\n",
    "        val_res, pert_data_first_half.adata[pert_data_first_half.adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    )\n",
    "    logger.info(f\"First half val_metrics at epoch {epoch}: \")\n",
    "    logger.info(val_metrics)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \")\n",
    "\n",
    "    val_score = val_metrics[\"pearson_de_delta\"]\n",
    "    if val_score > best_val_corr_first:\n",
    "        best_val_corr_first = val_score\n",
    "        best_model_first_half = copy.deepcopy(model_first_half)\n",
    "        logger.info(f\"Best first half model with score {val_score:5.4f}\")\n",
    "        patience_first = 0\n",
    "    else:\n",
    "        patience_first += 1\n",
    "        if patience_first >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    scheduler_first.step()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Second half model training\n",
    "best_val_loss_second = float(\"inf\")\n",
    "best_val_corr_second = 0\n",
    "best_model_second = None\n",
    "patience_second = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loader = pert_data_second_half.dataloader[\"train_loader\"]\n",
    "    valid_loader = pert_data_second_half.dataloader[\"val_loader\"]\n",
    "\n",
    "    train(\n",
    "        model_second_half,\n",
    "        train_loader,\n",
    "        epoch,\n",
    "        device,\n",
    "        criterion,\n",
    "        optimizer_second,\n",
    "        scheduler_second,\n",
    "        scaler_second,\n",
    "        n_genes,\n",
    "        gene_ids,\n",
    "        include_zero_gene,\n",
    "        max_seq_len,\n",
    "        CLS,\n",
    "        CCE,\n",
    "        MVC,\n",
    "        ECS,\n",
    "        log_interval,\n",
    "        logger\n",
    "    )\n",
    "\n",
    "    val_res = eval_perturb(valid_loader, model_second_half, device, include_zero_gene, gene_ids)\n",
    "    val_metrics = compute_perturbation_metrics(\n",
    "        val_res, pert_data_second_half.adata[pert_data_second_half.adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    )\n",
    "    logger.info(f\"Second half val_metrics at epoch {epoch}: \")\n",
    "    logger.info(val_metrics)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \")\n",
    "\n",
    "    val_score = val_metrics[\"pearson_de_delta\"]\n",
    "    if val_score > best_val_corr_second:\n",
    "        best_val_corr_second = val_score\n",
    "        best_model_second_half = copy.deepcopy(model_second_half)\n",
    "        logger.info(f\"Best second half model with score {val_score:5.4f}\")\n",
    "        patience_second = 0\n",
    "    else:\n",
    "        patience_second += 1\n",
    "        if patience_second >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    scheduler_second.step()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================SCGPT PREDICTIONS ===================\n",
    "\n",
    "scgpt_gene_indices = [adata.var.index.get_loc(gene) for gene in adata.var.index if gene in adata_gears.var.index]\n",
    "\n",
    "first_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in first_half_perturbations if gene != 'ctrl']\n",
    "second_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in second_half_perturbations if gene != 'ctrl']\n",
    "\n",
    "predictions_first_half = predict(\n",
    "    best_model_second_half,\n",
    "    first_half_perturbations_without_ctrl,\n",
    "    pert_data_first_half,\n",
    "    gene_ids=gene_ids,\n",
    "    include_zero_gene=include_zero_gene,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    amp=amp, pool_size=64\n",
    ")\n",
    "predictions_second_half = predict(\n",
    "    best_model_first_half,\n",
    "    second_half_perturbations_without_ctrl,\n",
    "    pert_data_second_half,\n",
    "    gene_ids=gene_ids,\n",
    "    include_zero_gene=include_zero_gene,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    amp=amp, pool_size=64\n",
    ")\n",
    "\n",
    "# Combine predictions from both halves\n",
    "scgpt_predictions = {}\n",
    "for pert in tqdm(predictions_first_half.keys()):\n",
    "    scgpt_predictions[pert] = predictions_first_half[pert][scgpt_gene_indices]\n",
    "for pert in tqdm(predictions_second_half.keys()):\n",
    "    scgpt_predictions[pert] = predictions_second_half[pert][scgpt_gene_indices]\n",
    "scgpt_predictions['control'] = pert_means['control']\n",
    "\n",
    "# Save scGPT predictions to pickle file\n",
    "with open('../../data/scgpt_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(scgpt_predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
