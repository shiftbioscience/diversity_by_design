{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('../../data/norman19/norman19_processed.h5ad'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/norman19/norman19_processed.h5ad ../../data/norman19/norman19_processed.h5ad')\n",
    "\n",
    "if not os.path.exists('../../data/norman19/norman19_names_df_vsrest.pkl'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/norman19/norman19_names_df_vsrest.pkl ../../data/norman19/norman19_names_df_vsrest.pkl')\n",
    "\n",
    "if not os.path.exists('../../data/norman19/norman19_scores_df_vsrest.pkl'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/norman19/norman19_scores_df_vsrest.pkl ../../data/norman19/norman19_scores_df_vsrest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded names_df_vsrest\n",
      "Successfully loaded scores_df_vsrest\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the numpy files\n",
    "try:\n",
    "    names_df_vsrest = np.load('../../data/norman19/norman19_names_df_vsrest.pkl', allow_pickle=True)\n",
    "    print(\"Successfully loaded names_df_vsrest\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading names_df_vsrest: {e}\")\n",
    "\n",
    "try:\n",
    "    scores_df_vsrest = np.load('../../data/norman19/norman19_scores_df_vsrest.pkl', allow_pickle=True)\n",
    "    print(\"Successfully loaded scores_df_vsrest\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading scores_df_vsrest: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.stats import ranksums # Added ranksums\n",
    "import scienceplots\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # For finding the 'analyses' package\n",
    "from common import *\n",
    "\n",
    "\n",
    "DATASET_NAME = 'norman19'\n",
    "\n",
    "# Initialize analysis using the common function\n",
    "(\n",
    "    adata,\n",
    "    pert_means, # This is the dictionary from get_pert_means(adata) \n",
    "    total_mean_original,\n",
    "    ctrl_mean_original,\n",
    "    DATASET_NAME,\n",
    "    DATASET_CELL_COUNTS,\n",
    "    DATASET_PERTS_TO_SWEEP,\n",
    "    dataset_specific_subdir, # e.g. \"norman19\" or \"replogle22\"\n",
    "    DATA_CACHE_DIR, # Base cache dir, e.g., \"../../../data/\"\n",
    "    original_np_random_state,\n",
    "    ANALYSIS_DIR,\n",
    "    pert_normalized_abs_scores_vsrest,\n",
    "    pert_counts,\n",
    "    scores_df_vsrest,\n",
    "    names_df_vsrest,\n",
    ") = initialize_analysis(DATASET_NAME, 'modeling_with_gears')\n",
    "\n",
    "loss_weights_dict = {}\n",
    "for key in pert_normalized_abs_scores_vsrest.keys():\n",
    "    new_key = key+'+ctrl' if '+' not in key else key\n",
    "    loss_weights_dict[new_key] = 100*pert_normalized_abs_scores_vsrest.get(key).values\n",
    "\n",
    "adata.var['gene_name'] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GEARS data preparation...\n",
      "Creating data copy...\n",
      "Processing condition labels...\n",
      "Getting unique perturbations...\n",
      "Found 175 unique perturbations\n",
      "Splitting perturbations into halves...\n",
      "Found 91 single gene perturbations\n",
      "Found 84 two gene perturbations\n",
      "First half: 42 perturbations\n",
      "Second half: 42 perturbations\n",
      "\n",
      "Processing first half splits...\n",
      "First half splits - Train: 33, Val: 9\n",
      "\n",
      "Processing second half splits...\n",
      "Second half splits - Train: 33, Val: 9\n",
      "\n",
      "Preparing data for GEARS...\n",
      "Creating subsetted datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First half dataset size: 42240 cells\n",
      "Second half dataset size: 42240 cells\n",
      "\n",
      "Processing first half data with GEARS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "['LYL1+IER5L' 'IER5L+ctrl' 'KIAA1804+ctrl']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: 121/124 genes kept (3 removed)\n",
      "val split: 9/9 genes kept (0 removed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test split: 130/133 genes kept (3 removed)\n",
      "First half data processing complete\n",
      "\n",
      "Processing second half data with GEARS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "These perturbations are not in the GO graph and their perturbation can thus not be predicted\n",
      "['IER5L+ctrl' 'KIAA1804+ctrl']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: 122/124 genes kept (2 removed)\n",
      "val split: 9/9 genes kept (0 removed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dataloaders....\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test split: 131/133 genes kept (2 removed)\n",
      "Second half data processing complete\n",
      "\n",
      "GEARS data preparation completed successfully!\n",
      "Training GEARS model with mse loss and unweighted weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from gears import PertData\n",
    "from gears_icml import GEARS\n",
    "import pickle\n",
    "\n",
    "# Check if all prediction files exist\n",
    "prediction_files = [\n",
    "    '../../data/gears_predictions_mse_unweighted_norman19.pkl',\n",
    "    '../../data/gears_predictions_mse_weighted_norman19.pkl',\n",
    "    '../../data/gears_predictions_default_loss_unweighted_norman19.pkl',\n",
    "    '../../data/gears_predictions_default_loss_weighted_norman19.pkl'\n",
    "]\n",
    "\n",
    "if all(os.path.exists(f) for f in prediction_files):\n",
    "    print(\"All prediction files exist. Loading predictions...\")\n",
    "    gears_predictions = {}\n",
    "    for loss in ['default_loss', 'mse']:\n",
    "        for weight in ['unweighted', 'weighted']:\n",
    "            with open(f'../../data/gears_predictions_{loss}_{weight}_norman19.pkl', 'rb') as f:\n",
    "                gears_predictions[f'{loss}_{weight}'] = pickle.load(f)\n",
    "else:\n",
    "    # ===================GEARS DATA PREPARATION ===================\n",
    "    print(\"Starting GEARS data preparation...\")\n",
    "\n",
    "    gears_predictions = {}\n",
    "\n",
    "    # Create a copy of the original data and subsample cells\n",
    "    print(\"Creating data copy...\")\n",
    "    adata_gears = adata.copy()\n",
    "    np.random.seed(42)  # Set random seed for reproducibility\n",
    "\n",
    "    # Process condition labels\n",
    "    print(\"Processing condition labels...\")\n",
    "    adata_gears.obs['condition'] = adata_gears.obs['condition'].astype(str)\n",
    "    adata_gears.obs['condition'] = adata_gears.obs['condition'].apply(lambda x: x + '+ctrl' if '+' not in x else x)\n",
    "    adata_gears.obs['condition'] = adata_gears.obs['condition'].str.replace('control+ctrl', 'ctrl')\n",
    "\n",
    "    # Get unique perturbations\n",
    "    print(\"Getting unique perturbations...\")\n",
    "    all_perturbations = adata_gears.obs['condition'].unique()\n",
    "    all_perturbations = all_perturbations[all_perturbations != 'ctrl']\n",
    "    print(f\"Found {len(all_perturbations)} unique perturbations\")\n",
    "\n",
    "    # Split perturbations into first and second half\n",
    "    print(\"Splitting perturbations into halves...\")\n",
    "    # Split perturbations into single gene and two gene perturbations\n",
    "    single_gene_perturbations = np.array([p for p in all_perturbations if '+ctrl' in p])\n",
    "    two_gene_perturbations = np.array([p for p in all_perturbations if 'ctrl' not in p])\n",
    "\n",
    "    print(f\"Found {len(single_gene_perturbations)} single gene perturbations\")\n",
    "    print(f\"Found {len(two_gene_perturbations)} two gene perturbations\")\n",
    "\n",
    "    # Use single gene perturbations for the split\n",
    "    first_half_perturbations = two_gene_perturbations[:len(two_gene_perturbations)//2]\n",
    "    second_half_perturbations = two_gene_perturbations[len(two_gene_perturbations)//2:]\n",
    "    print(f\"First half: {len(first_half_perturbations)} perturbations\")\n",
    "    print(f\"Second half: {len(second_half_perturbations)} perturbations\")\n",
    "\n",
    "    # Process first half splits\n",
    "    print(\"\\nProcessing first half splits...\")\n",
    "    first_half_train_val = first_half_perturbations\n",
    "    np.random.shuffle(first_half_train_val)\n",
    "    split_idx = int(len(first_half_train_val) * 0.8) # 80% train, 10% val\n",
    "    first_half_train = first_half_train_val[:split_idx]\n",
    "    first_half_val = first_half_train_val[split_idx:]\n",
    "\n",
    "    first_half_split_dict = {\n",
    "        'train': np.concatenate([first_half_train, single_gene_perturbations]),\n",
    "        'val': first_half_val,\n",
    "        'test': np.concatenate([first_half_train_val, single_gene_perturbations])\n",
    "    }\n",
    "    print(f\"First half splits - Train: {len(first_half_train)}, Val: {len(first_half_val)}\")\n",
    "\n",
    "    # Process second half splits\n",
    "    print(\"\\nProcessing second half splits...\")\n",
    "    second_half_train_val = second_half_perturbations\n",
    "    np.random.shuffle(second_half_train_val) \n",
    "    split_idx = int(len(second_half_train_val) * 0.8)  # 80% train, 10% val\n",
    "    second_half_train = second_half_train_val[:split_idx]\n",
    "    second_half_val = second_half_train_val[split_idx:]\n",
    "\n",
    "    second_half_split_dict = {\n",
    "        'train': np.concatenate([second_half_train, single_gene_perturbations]),\n",
    "        'val': second_half_val,\n",
    "        'test': np.concatenate([second_half_train_val, single_gene_perturbations])\n",
    "    }\n",
    "    print(f\"Second half splits - Train: {len(second_half_train)}, Val: {len(second_half_val)}\")\n",
    "\n",
    "    # Prepare data for GEARS\n",
    "    print(\"\\nPreparing data for GEARS...\")\n",
    "    first_half_perturbations = np.concatenate([['ctrl'], first_half_perturbations, single_gene_perturbations])\n",
    "    second_half_perturbations = np.concatenate([['ctrl'], second_half_perturbations, single_gene_perturbations])\n",
    "\n",
    "    # Create subsetted datasets\n",
    "    print(\"Creating subsetted datasets...\")\n",
    "    adata_first_half_gears = adata_gears[adata_gears.obs['condition'].isin(first_half_perturbations)].copy()\n",
    "    adata_second_half_gears = adata_gears[adata_gears.obs['condition'].isin(second_half_perturbations)].copy()\n",
    "    print(f\"First half dataset size: {adata_first_half_gears.n_obs} cells\")\n",
    "    print(f\"Second half dataset size: {adata_second_half_gears.n_obs} cells\")\n",
    "\n",
    "    # Process first half data\n",
    "    print(\"\\nProcessing first half data with GEARS...\")\n",
    "    pert_data_first_half = PertData('../../data')\n",
    "    if not os.path.exists('../../data/norman19_gears_first_half_gears'):\n",
    "        pert_data_first_half.new_data_process(dataset_name='norman19_gears_first_half_gears', adata=adata_first_half_gears)\n",
    "    pert_data_first_half.load(data_path='../../data/norman19_gears_first_half_gears')\n",
    "    # Filter out genes not in gene2go from each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        original_count = len(first_half_split_dict[split])\n",
    "        first_half_split_dict[split] = [gene for gene in first_half_split_dict[split] \n",
    "                                    if gene.split('+')[0] in ['ctrl'] + list(pert_data_first_half.gene2go.keys()) and gene.split('+')[1] in ['ctrl'] + list(pert_data_first_half.gene2go.keys())]\n",
    "        filtered_count = len(first_half_split_dict[split])\n",
    "        print(f\"{split} split: {filtered_count}/{original_count} genes kept ({original_count - filtered_count} removed)\")\n",
    "    with open('../../data/norman19_gears_first_half_split_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(first_half_split_dict, f)\n",
    "    pert_data_first_half.prepare_split(split='custom', seed=42, split_dict_path='../../data/norman19_gears_first_half_split_dict.pkl')\n",
    "    print(\"First half data processing complete\")\n",
    "\n",
    "    # Process second half data\n",
    "    print(\"\\nProcessing second half data with GEARS...\")\n",
    "    pert_data_second_half = PertData('../../data')\n",
    "    if not os.path.exists('../../data/norman19_gears_second_half_gears'):\n",
    "        pert_data_second_half.new_data_process(dataset_name='norman19_gears_second_half_gears', adata=adata_second_half_gears)\n",
    "    pert_data_second_half.load(data_path='../../data/norman19_gears_second_half_gears')\n",
    "    # Filter out genes not in gene2go from each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        original_count = len(second_half_split_dict[split])\n",
    "        second_half_split_dict[split] = [gene for gene in second_half_split_dict[split] \n",
    "                                    if gene.split('+')[0] in ['ctrl'] + list(pert_data_second_half.gene2go.keys()) and gene.split('+')[1] in ['ctrl'] + list(pert_data_second_half.gene2go.keys())]\n",
    "        filtered_count = len(second_half_split_dict[split])\n",
    "        print(f\"{split} split: {filtered_count}/{original_count} genes kept ({original_count - filtered_count} removed)\")\n",
    "    with open('../../data/norman19_gears_second_half_split_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(second_half_split_dict, f)\n",
    "    pert_data_second_half.prepare_split(split='custom', seed=42, split_dict_path='../../data/norman19_gears_second_half_split_dict.pkl')\n",
    "    print(\"Second half data processing complete\")\n",
    "\n",
    "    print(\"\\nGEARS data preparation completed successfully!\")\n",
    "\n",
    "    # Get dataloaders\n",
    "    pert_data_first_half.get_dataloader(batch_size = 32, test_batch_size = 512)\n",
    "    pert_data_second_half.get_dataloader(batch_size = 32, test_batch_size = 512)\n",
    "\n",
    "    # Prepare perturbations for prediction\n",
    "    first_half_train_val_without_ctrl = [gene.split('+') for gene in first_half_train_val if gene.split('+')[0] in pert_data_first_half.gene2go.keys() and gene.split('+')[1] in pert_data_first_half.gene2go.keys()]\n",
    "    second_half_train_val_without_ctrl = [gene.split('+') for gene in second_half_train_val if gene.split('+')[0] in pert_data_first_half.gene2go.keys() and gene.split('+')[1] in pert_data_first_half.gene2go.keys()]\n",
    "    \n",
    "    # Train and evaluate models with different loss functions and weights\n",
    "    losses = ['mse', 'default_loss']\n",
    "    weights = ['unweighted', 'weighted']\n",
    "\n",
    "    for loss in losses:\n",
    "\n",
    "        for weight in weights:\n",
    "\n",
    "            if not os.path.exists(f'../../data/gears_predictions_{loss}_{weight}_norman19.pkl'):\n",
    "\n",
    "                print(f\"Training GEARS model with {loss} loss and {weight} weight\")\n",
    "\n",
    "                # ===================GEARS MODEL TRAINING ===================\n",
    "\n",
    "                gears_model_first_half = GEARS(pert_data_first_half, device = 'cuda', \n",
    "                                        weight_bias_track = False, \n",
    "                                        proj_name = 'first_half', \n",
    "                                        exp_name = 'first_half',\n",
    "                                        loss_weights_dict = loss_weights_dict if weight == 'weighted' else None,\n",
    "                                        use_mse_loss = True if loss == 'mse' else False)\n",
    "                gears_model_first_half.model_initialize()\n",
    "\n",
    "                gears_model_second_half = GEARS(pert_data_second_half, device = 'cuda', \n",
    "                                        weight_bias_track = False, \n",
    "                                        proj_name = 'second_half', \n",
    "                                        exp_name = 'second_half',\n",
    "                                        loss_weights_dict = loss_weights_dict if weight == 'weighted' else None,\n",
    "                                        use_mse_loss = True if loss == 'mse' else False)\n",
    "                gears_model_second_half.model_initialize()\n",
    "\n",
    "                gears_model_first_half.train(epochs = 10)\n",
    "                gears_model_second_half.train(epochs = 10)\n",
    "\n",
    "                os.makedirs(\"../../data/gears_models\", exist_ok=True)\n",
    "\n",
    "                gears_model_first_half.save_model(f'../../data/gears_models/norman19_first_half_{loss}_{weight}')\n",
    "                gears_model_second_half.save_model(f'../../data/gears_models/norman19_second_half_{loss}_{weight}')\n",
    "\n",
    "                # ===================GEARS PREDICTIONS ===================\n",
    "\n",
    "                # Compile GEARS predictions\n",
    "                predictions_first_half = gears_model_second_half.predict(first_half_train_val_without_ctrl)\n",
    "                predictions_second_half = gears_model_first_half.predict(second_half_train_val_without_ctrl)\n",
    "\n",
    "                # Combine predictions from both halves\n",
    "                gears_predictions[f'{loss}_{weight}'] = {}\n",
    "                for pert in tqdm(predictions_first_half.keys()):\n",
    "                    gears_predictions[f'{loss}_{weight}'][pert.replace('_', '+')] = predictions_first_half[pert]\n",
    "                for pert in tqdm(predictions_second_half.keys()):\n",
    "                    gears_predictions[f'{loss}_{weight}'][pert.replace('_', '+')] = predictions_second_half[pert]\n",
    "                gears_predictions[f'{loss}_{weight}']['control'] = pert_means['control']\n",
    "\n",
    "                # Save GEARS predictions to pickle file\n",
    "                with open(f'../../data/gears_predictions_{loss}_{weight}_norman19.pkl', 'wb') as f:\n",
    "                    pickle.dump(gears_predictions[f'{loss}_{weight}'], f)\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Load GEARS predictions from pickle file\n",
    "                with open(f'../../data/gears_predictions_{loss}_{weight}_norman19.pkl', 'rb') as f:\n",
    "                    gears_predictions[f'{loss}_{weight}'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
