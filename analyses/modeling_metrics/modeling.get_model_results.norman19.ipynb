{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, create a conda environment with scgpt and other deps installed.\n",
    "\n",
    "```bash\n",
    "conda create -y -n scgpt python=3.11\n",
    "conda activate scgpt\n",
    "# pip install -r requirements.txt\n",
    "pip install scgpt\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('../../data/replogle22/replogle22_processed.h5ad'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/replogle22/replogle22_processed.h5ad ../../data/replogle22/replogle22_processed.h5ad')\n",
    "\n",
    "if not os.path.exists('../../data/replogle22/replogle22_names_df_vsrest.pkl'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/replogle22/replogle22_names_df_vsrest.pkl ../../data/replogle22/replogle22_names_df_vsrest.pkl')\n",
    "\n",
    "if not os.path.exists('../../data/replogle22/replogle22_scores_df_vsrest.pkl'):\n",
    "    os.system('aws s3 cp s3://shift-personal-dev/henry/icml_data/replogle22/replogle22_scores_df_vsrest.pkl ../../data/replogle22/replogle22_scores_df_vsrest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded names_df_vsrest\n",
      "Successfully loaded scores_df_vsrest\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the numpy files\n",
    "try:\n",
    "    names_df_vsrest = np.load('../../data/replogle22/replogle22_names_df_vsrest.pkl', allow_pickle=True)\n",
    "    print(\"Successfully loaded names_df_vsrest\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading names_df_vsrest: {e}\")\n",
    "\n",
    "try:\n",
    "    scores_df_vsrest = np.load('../../data/replogle22/replogle22_scores_df_vsrest.pkl', allow_pickle=True)\n",
    "    print(\"Successfully loaded scores_df_vsrest\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading scores_df_vsrest: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating perturbation means: 100%|██████████| 1335/1335 [00:06<00:00, 198.60it/s]\n",
      "Calculating WMSE Weights: 100%|██████████| 1335/1335 [00:01<00:00, 886.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.stats import ranksums # Added ranksums\n",
    "import scienceplots\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # For finding the 'analyses' package\n",
    "from common import *\n",
    "\n",
    "\n",
    "DATASET_NAME = 'replogle22'\n",
    "\n",
    "# Initialize analysis using the common function\n",
    "(\n",
    "    adata,\n",
    "    pert_means, # This is the dictionary from get_pert_means(adata) \n",
    "    total_mean_original,\n",
    "    ctrl_mean_original,\n",
    "    DATASET_NAME,\n",
    "    DATASET_CELL_COUNTS,\n",
    "    DATASET_PERTS_TO_SWEEP,\n",
    "    dataset_specific_subdir, # e.g. \"norman19\" or \"replogle22\"\n",
    "    DATA_CACHE_DIR, # Base cache dir, e.g., \"../../../data/\"\n",
    "    original_np_random_state,\n",
    "    ANALYSIS_DIR,\n",
    "    pert_normalized_abs_scores_vsrest,\n",
    "    pert_counts,\n",
    "    scores_df_vsrest,\n",
    "    names_df_vsrest,\n",
    ") = initialize_analysis(DATASET_NAME, 'modeling_with_gears')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['control', 'TFAM', 'SLC1A5', 'GFM1', 'MRPL36', 'TARDBP', 'PPP6C', 'MRPL35', 'NBPF12', 'CCDC6', 'GTF3C4', 'BCAR1', 'MYBL2', 'FBXO42', 'HSD17B10', 'POGLUT3', 'GAB2', 'CLOCK', 'TMEM214', 'TWF1', 'EPS8L1', 'TMEM242', 'TACC3', 'TPT1', 'KRT17', 'ANAPC15', 'FAM136A', 'SPC25', 'CCDC78', 'SMG5', 'IK', 'SSBP3', 'ZNF718', 'SMN2', 'MVK', 'BCL2L1', 'ZDHHC7', 'DDX19A', 'EIF4B', 'SLC35G2', 'TREX2', 'WTAP', 'ADAM10', 'RGPD6', 'C7orf26', 'LAMTOR1', 'DDX19B', 'PPP1R37', 'INTS14', 'TRNT1', 'ESPN', 'CCNK', 'TIMM23B', 'RHOQ', 'CLASRP', 'PCBP2', 'PMF1', 'EIF3CL', 'HSD17B12', 'CSH2', 'DNM1', 'DNAAF3', 'SRSF11', 'CENPT', 'NAA35', 'TFRC', 'SNX15', 'TBX1', 'RTEL1', 'INTS13', 'UBTF', 'PPP2R1A', 'PRODH', 'GYG1', 'PARS2', 'C14orf178', 'PSTK', 'ADAT3', 'MRPL39', 'ZBTB17', 'ATP6AP2', 'ZFP69B', 'YRDC', 'GTF2E2', 'UBE2M', 'BTF3L4', 'TUBB', 'CSE1L', 'H2AFZ', 'VPS41', 'DSTYK', 'LMO2', 'DDN', 'PSMG3', 'MRPL38', 'ANKS6', 'MRPL16', 'HMGCS1', 'NAGLU', 'TKT', 'GLRX5', 'RBM14-RBM4', 'SIRT7', 'SSU72', 'SLC7A5', 'ATP6V0C', 'MRPS21', 'UROD', 'CCDC144NL', 'CADM4', 'MCL1', 'WDR54', 'CACNB3', 'HIRA', 'DYNLRB1', 'GPS1', 'SART3', 'HIST1H2BC', 'DPH1', 'YJEFN3', 'JAZF1', 'HSCB', 'CNN2', 'TRAPPC11', 'ACTB', 'LTBP3', 'MAD2L2', 'POLR1C', 'VPS54', 'PKM', 'ATP5MF', 'HUS1', 'RAB4B', 'RRM2', 'SNRNP40', 'RSL1D1', 'MRPS16', 'NAPG', 'POU5F1B', 'SEC16A', 'DDX46', 'MFN2', 'NARS2', 'AP2M1', 'SLC25A10', 'TUT1', 'DYNLL2', 'EFR3A', 'FOLR3', 'MRPL41', 'RHPN1', 'ZRSR2', 'MRPL24', 'KRT8', 'METTL3', 'MTG2', 'ARL2', 'ZMAT2', 'C1orf109', 'KDM6A', 'RNF8', 'RBM15', 'MRPS6', 'PDCD6IP', 'POLRMT', 'ALG1L', 'KPNB1', 'HHEX', 'FAM207A', 'CPAMD8', 'ZNF706', 'TUBA1B', 'TSR1', 'ATP5PD', 'LZTR1', 'CDK2', 'PTEN', 'FAM229A', 'PCBP1', 'YTHDC1', 'MBD3', 'FBXW7', 'DDX5', 'ACOT12', 'KRT10', 'MED19', 'ZNF133', 'QRSL1', 'CDK6', 'GCLC', 'RTRAF', 'FBLIM1', 'PMPCB', 'NXF1', 'COQ2', 'SP1', 'SDHAF2', 'NPLOC4', 'COX5A', 'AQP7', 'SNW1', 'SRPRB', 'OR1E2', 'MRPL27', 'PTMA', 'RABGGTA', 'MBTPS1', 'CD8B', 'TARS2', 'GNPNAT1', 'C17orf58', 'CPSF3', 'USP14', 'UQCRC1', 'DPY19L4', 'ABHD17A', 'MCM3', 'IST1', 'MKRN1', 'PRKRIP1', 'TAF13', 'KDM1A', 'GRB2', 'RCC1', 'PRPF39', 'RPL41', 'CPNE7', 'SUPT20H', 'JMJD6', 'MICOS10', 'DDX23', 'RNF14', 'DHODH', 'PGAM5', 'LIMS1', 'C16orf86', 'NDUFA3', 'RAB4A', 'DDX17', 'ATP6AP1', 'MANF', 'CYS1', 'EIF4G2', 'PPP1CA', 'COX7C', 'SMAGP', 'DNAJA1', 'TUFM', 'TSEN34', 'PCNX3', 'ACTR1B', 'GET3', 'PTPN11', 'TMED10', 'MON1A', 'SRSF6', 'THOC7', 'POLR2I', 'USP5', 'PLK4', 'EHMT2', 'TXNL4B', 'HNRNPH1', 'POLR2M', 'CKAP5', 'STAT5B', 'TFDP1', 'CABIN1', 'RNF123', 'ZNF100', 'SRA1', 'SBNO1', 'HAUS8', 'HAMP', 'PRMT5', 'MRPL34', 'TOX4', 'ELL', 'MALSU1', 'PCID2', 'NBEAL1', 'CASC3', 'GCOM1', 'PPME1', 'EIF4H', 'SEPSECS', 'MRPL3', 'MRPS5', 'WBP11', 'ZNF574', 'SON', 'NISCH', 'COX10', 'TUBGCP2', 'NDUFB4', 'TNNT2', 'BOP1', 'MRPL33', 'SS18L2', 'UXS1', 'SF3B3', 'ANKRD11', 'EIF1', 'MRPL54', 'ORC4', 'TBC1D28', 'SRP72', 'XPO5', 'LETM1', 'KEAP1', 'ATP6V1B2', 'LIN54', 'ASF1B', 'ACIN1', 'OTOP1', 'PHAX', 'EGLN2', 'ATR', 'TBCA', 'DHX30', 'NPEPPS', 'CAMLG', 'CENPA', 'CMPK1', 'TM7SF2', 'PSAT1', 'MRPS17', 'MAT2A', 'C9orf16', 'DCTN3', 'DNAJA3', 'ZNF253', 'NRBP1', 'ABCF1', 'LST1', 'UBA2', 'PARN', 'UBA52', 'E2F6', 'PPIL1', 'SF3A2', 'IMPDH2', 'DCTN1', 'PRPF31', 'HAUS4', 'SNF8', 'LRRC37A3', 'PET117', 'MIER1', 'PRRC2A', 'RNF20', 'NDUFB8', 'MBTPS2', 'NBPF15', 'ELOF1', 'INTS8', 'NELFB', 'EMC3', 'DNAJC8', 'MRPL4', 'THAP1', 'FYN', 'TUBB2A', 'PNISR', 'PRORP', 'SRSF7', 'INO80C', 'NEDD1', 'NFRKB', 'TIMM8A', 'PRSS50', 'SHQ1', 'PNPT1', 'PXN', 'NFATC2IP', 'RAD9A', 'GOLT1B', 'HSPA8', 'NUB1', 'EIF2B2', 'DRAP1', 'THAP11', 'PSMB5', 'DERL2', 'CHTOP', 'SIN3A', 'THOC6', 'NAA25', 'MRPL1', 'ISCA2', 'CHORDC1', 'COPZ1', 'PPP1R8', 'SPCS3', 'ARL4D', 'PPA1', 'TP53I13', 'HSP90B1', 'RNASEH2C', 'SOD2', 'ATP2A2', 'WDR70', 'VPS37A', 'DNLZ', 'TMX2', 'TEAD3', 'NUP107', 'SHC1', 'UBE3D', 'SMG7', 'STXBP4', 'SRP9', 'PAXBP1', 'HS6ST1', 'SLC39A10', 'MRPL15', 'COMTD1', 'DAP3', 'SMARCB1', 'KIAA1143', 'WDR4', 'MYO1H', 'GMIP', 'INTS12', 'CFL1', 'PDCD5', 'CCDC144A', 'EMC7', 'FNTB', 'MRPL50', 'ARPC2', 'TUBG1', 'CENPW', 'ATP5PO', 'ACTR1A', 'TADA1', 'MRPS10', 'GTF3C6', 'GUK1', 'NDUFB3', 'PITRM1', 'SMG6', 'SCD', 'CHCHD4', 'MRPS27', 'MRPS25', 'H3F3A', 'VPS35', 'RBM10', 'ATP5ME', 'OIP5', 'PYROXD1', 'RARS2', 'RPS29', 'COA5', 'NAA30', 'ATP6V1G1', 'OLFML3', 'DDX1', 'PC', 'ELOB', 'PPP2R3C', 'ABCG1', 'NUDCD3', 'TPRKB', 'ZNF207', 'HNRNPA1', 'ING3', 'GRSF1', 'DCAF6', 'SUDS3', 'HMGB1', 'ZNF292', 'POU3F1', 'CHD4', 'ERVW-1', 'PRELID3B', 'COX6C', 'RPS13', 'TLCD1', 'TPR', 'MIOS', 'LAMTOR4', 'NPB', 'KTI12', 'THRAP3', 'DBR1', 'GAPDH', 'DRG1', 'TEN1', 'ZFR', 'RAC3', 'KIF14', 'EPB41L2', 'ERAL1', 'EARS2', 'UBE2L3', 'LARS2', 'MRPS26', 'STRIP1', 'MRPS11', 'ATP11B', 'PFN1', 'CCNC', 'DNAJC19', 'STAG2', 'ASCC3', 'MRPL14', 'OTX1', 'DPH6', 'POGZ', 'RPP21', 'GPS2', 'ADSL', 'ZNF284', 'ZNF674', 'PHF10', 'LTB4R2', 'SPG7', 'SNAPC3', 'GFOD2', 'UBL5', 'TTC4', 'TBCE', 'CBX1', 'RAD51D', 'SRCAP', 'DDB1', 'MCM6', 'USP9X', 'MSRB1', 'KRTAP4-7', 'SEH1L', 'SLC35G6', 'NSUN4', 'TIMM13', 'FAM50A', 'TUBD1', 'SETX', 'EIF3B', 'PLA2G10', 'NCAPD2', 'DAXX', 'SETDB1', 'RPL7', 'CMC4', 'TTC1', 'MRPL53', 'USP19', 'USP39', 'GON4L', 'DOLK', 'MZF1', 'GOLGA6L1', 'PELO', 'UBE2N', 'WBP1L', 'NFYC', 'CYCS', 'NDUFA11', 'GARS', 'EIF3L', 'KLHL11', 'DOHH', 'HJURP', 'DDX54', 'NDUFS3', 'CHTF18', 'PGAM1', 'UQCRB', 'TUBE1', 'PDCD6', 'CSTF1', 'FASTKD5', 'PPIA', 'SRSF10', 'TADA2A', 'ARID3A', 'RUVBL1', 'CCAR1', 'ARID5B', 'HIST2H3D', 'SFPQ', 'NKAPD1', 'MRPS30', 'MRPS7', 'METTL17', 'TAF11', 'DDX42', 'FAF2', 'MRPL10', 'IPO9', 'FOXD4', 'KLHL17', 'HAUS7', 'TRMT6', 'INF2', 'COTL1', 'SLC39A9', 'LTBP4', 'POTEI', 'VPS37C', 'DMRTA2', 'ANAPC13', 'EIF2B5', 'MAGOH', 'ORC5', 'FNBP4', 'FDXR', 'EWSR1', 'COX11', 'PRKCA', 'ARFRP1', 'NDUFA4', 'TOMM22', 'LSM12', 'AIFM1', 'ALG14', 'WAC', 'NOMO3', 'CA5A', 'ATP6V0B', 'UFL1', 'TAMM41', 'CCDC130', 'GSDMA', 'MEIS3', 'BAP1', 'RPLP0', 'DCTN4', 'NUP88', 'IWS1', 'NELFCD', 'UPF2', 'GET1', 'PRPF3', 'HSPA9', 'NUP153', 'HIST1H2BM', 'DNMT1', 'RPUSD3', 'RPL35A', 'CT45A5', 'TNRC6A', 'ENO1', 'ZC3H13', 'S100A1', 'HNRNPL', 'PGK1', 'ZNF687', 'UNC45A', 'CBFA2T3', 'TFB1M', 'VEZT', 'ZNF763', 'SPEN', 'HIST2H2BE', 'WDR77', 'TRAPPC5', 'NDC1', 'MRPL28', 'BTF3', 'TIMM10', 'BRIP1', 'RPA3', 'ELP3', 'RPL23', 'INPPL1', 'MRPL19', 'NDUFA6', 'RABGGTB', 'NUP133', 'SARS2', 'MRPL20', 'H2AFX', 'EEF2', 'NDUFS5', 'COPG1', 'WNK1', 'HIST1H2BB', 'ATP5F1B', 'VPS25', 'MYSM1', 'GTF3C5', 'PRDM4', 'HNRNPM', 'MAD2L1', 'CCNB1', 'SNRPC', 'TFB2M', 'POLD2', 'ARGFX', 'SLC7A6OS', 'LRP5', 'SLC4A5', 'DTYMK', 'XRCC1', 'PPP2CA', 'UHRF1', 'MMGT1', 'CHMP6', 'PSMC5', 'NUP98', 'CFDP1', 'NDUFB7', 'BCS1L', 'PSME2', 'NCOA4', 'BUB1B', 'TBPL1', 'CDC16', 'HIST1H2BL', 'MRPL46', 'CDT1', 'ELOVL1', 'METTL21A', 'MLLT6', 'USF2', 'TRMT10C', 'CS', 'GNB1L', 'LCE1E', 'HSPA14', 'ARHGAP11B', 'ARGLU1', 'UBR5', 'RBMX2', 'NDUFAF3', 'SART1', 'C6orf15', 'IRF2BP2', 'SGPP1', 'METTL1', 'TVP23C', 'SPTSSA', 'ACTR6', 'ELAC2', 'ZNF506', 'EFTUD2', 'EIF2B3', 'CNIH4', 'KDM5C', 'MRPS35', 'NASP', 'PTPMT1', 'ELP5', 'KLC2', 'ZNF468', 'MRPS28', 'CIAO1', 'LSM10', 'SERBP1', 'CUL3', 'ZC3H3', 'MTOR', 'PABPC1', 'PSME1', 'TUBGCP4', 'PGD', 'ATP1A1', 'HSPE1', 'LCE1C', 'ZNF236', 'MRPS23', 'RTTN', 'EIF2B1', 'DCTN2', 'ZNF492', 'GRPEL1', 'TSFM', 'BPTF', 'SUPV3L1', 'OVCA2', 'SNRNP200', 'GMPPB', 'TEFM', 'WDR83OS', 'TRA2B', 'GBF1', 'KCTD10', 'MEN1', 'RCCD1', 'NDC80', 'PFDN4', 'MED14', 'CNOT1', 'DNM1L', 'MRPL51', 'ATP6V0D1', 'CIAPIN1', 'PTPN1', 'TULP1', 'TAZ', 'VPS29', 'RBM33', 'TMEM127', 'MRPL18', 'NAA20', 'PTGR2', 'MTHFD1', 'NELFE', 'YARS', 'BCR', 'NDUFB10', 'SEM1', 'ERCC2', 'UBE2H', 'CTBP2', 'BOD1L1', 'TIGD1', 'VPS13D', 'C9orf78', 'UBA1', 'YEATS4', 'PGS1', 'KRTAP4-2', 'SNRNP27', 'COG6', 'MRPL17', 'PRPF19', 'EDC4', 'CAD', 'TUBGCP5', 'GEMIN5', 'SMG9', 'SAE1', 'GJA3', 'GSK3B', 'TSR2', 'TSSK3', 'STAT5A', 'MRPL55', 'RFT1', 'DPY19L2', 'RPL10A', 'HDAC7', 'SRF', 'TWNK', 'DHX8', 'MCMBP', 'PKMYT1', 'DDX11', 'PPP2CB', 'AARS2', 'NCL', 'SMC1A', 'BRF2', 'ESF1', 'ETF1', 'PSMA4', 'EXOC5', 'MLST8', 'OR4K1', 'MRPS33', 'CHCHD2', 'TMEM240', 'CLCC1', 'DICER1', 'CYP2A13', 'HMGB3', 'SPRTN', 'MRPL44', 'SRP14', 'C7orf50', 'NOP56', 'LCN10', 'MRPS18A', 'CXXC1', 'XRCC2', 'DDX47', 'POLD1', 'ZNF317', 'ANKRD49', 'ATP5F1A', 'WDR44', 'POLG2', 'YARS2', 'MRPS9', 'NUBP2', 'CYC1', 'RAB6A', 'INTS10', 'HERC1', 'ARMC6', 'CNOT10', 'CRCP', 'CDCA5', 'MRPL43', 'DEPDC5', 'FBXL14', 'MARS2', 'CYP4F11', 'METTL23', 'HAUS5', 'CACTIN', 'TP53RK', 'NAA38', 'TAF1D', 'MRPL37', 'SEC61A1', 'SMG8', 'SMARCC1', 'VPS72', 'PRPF8', 'MAX', 'RBM12', 'HOXC10', 'RBMXL1', 'VARS2', 'DNAJC11', 'DHX9', 'DPPA2', 'AHCY', 'OR2T5', 'DPM2', 'LRPPRC', 'MMS19', 'SETD5', 'CDC6', 'ACTR8', 'TBC1D3', 'CAPZB', 'LRRC37B', 'USP7', 'PNN', 'SF3A1', 'FOXS1', 'SAMD4B', 'HNRNPK', 'ENY2', 'MOK', 'HARS', 'SERPINB1', 'SBDS', 'EMC4', 'PPIL4', 'DCUN1D5', 'SNAPC4', 'EEF1B2', 'SLC9B1', 'DBF4', 'POLR2E', 'VARS', 'ATIC', 'UBE3C', 'MYC', 'MTBP', 'YY1', 'WDR7', 'PIAS4', 'HIST1H2BN', 'HECTD1', 'EXOC2', 'IARS2', 'AAAS', 'CDK1', 'URI1', 'UCHL5', 'TSEN2', 'F8A1', 'ADRM1', 'HIST1H2AE', 'PTK2', 'PRPF4', 'AFG3L2', 'ZFC3H1', 'SCNM1', 'MRPL23', 'POLR2F', 'TBP', 'TRNP1', 'LSM5', 'NDUFV2', 'PLEKHN1', 'RANGAP1', 'PHB', 'ARPC4', 'ANAPC11', 'PABPC4', 'OR7A17', 'NFYB', 'CEP68', 'COX5B', 'ELP6', 'MRPL9', 'SRRT', 'MTPAP', 'BRCA2', 'SDC1', 'GLRX3', 'SETD1A', 'GLE1', 'VBP1', 'SPDYE2', 'ZMYM4', 'BRD2', 'SMC4', 'EIF2S1', 'NBPF3', 'TADA2B', 'CCND3', 'NKAP', 'UMPS', 'U2AF1', 'TRAPPC1', 'CTU2', 'CMTR1', 'CDK11A', 'CLPB', 'TXN', 'SF3B5', 'MNAT1', 'DHX15', 'EIF3M', 'SMU1', 'ALG1', 'HNRNPU', 'PSMC1', 'GEMIN4', 'MIPEP', 'PUM1', 'LONP1', 'PPP1R2', 'KXD1', 'POLR2D', 'BCLAF1', 'NCKAP1', 'SMC2', 'CDIPT', 'GINS4', 'REXO2', 'RPL12', 'FAM102B', 'SEPHS2', 'PPA2', 'LDB1', 'NDUFB6', 'ZNHIT2', 'KDM8', 'RCOR1', 'FBRSL1', 'INO80E', 'TTI1', 'WDR18', 'ACSS2', 'TMSB10', 'ZCRB1', 'MRPS24', 'COX17', 'CENPK', 'VPS18', 'CBLN1', 'TOMM40', 'SMNDC1', 'NUP54', 'CTNNBL1', 'TTK', 'EIF3I', 'HIST2H3A', 'INO80B', 'ARPC3', 'QARS', 'GTF3C2', 'GAK', 'LSM4', 'KIF4A', 'FOXL2', 'RPL36', 'PAM16', 'DENR', 'NOS1AP', 'ZBTB44', 'AHCTF1', 'CCT6A', 'SRRM2', 'MRPL2', 'ATP6V1H', 'RFC2', 'RBX1', 'ATP6V1A', 'NEMF', 'SIGLEC14', 'PHB2', 'SCAP', 'RPL10', 'RBBP5', 'ZNHIT3', 'CSNK2B', 'COQ5', 'STARD7', 'CENPN', 'TUBA1C', 'TMEM199', 'ALG2', 'CLK2', 'USP10', 'TEX10', 'DDX55', 'SRRM1', 'SLU7', 'MST1', 'MRPS2', 'NUP85', 'EIF4G1', 'DHPS', 'TRAPPC3', 'ARCN1', 'HUWE1', 'POLG', 'OR1D2', 'MEMO1', 'IPO11', 'PPARGC1B', 'YPEL1', 'BRD4', 'TMED2', 'SYNGR3', 'SSX4', 'SNRNP48', 'DLD', 'NHLRC2', 'ANAPC1', 'NDUFAB1', 'METTL14', 'MED4', 'TAF8', 'ESYT1', 'RPL6', 'SDHA', 'YPEL5', 'RPL4', 'SEC63', 'RPA2', 'MOCS3', 'RNF103', 'RPL37A', 'RPL32', 'RPL11', 'PAF1', 'HSPA5', 'IPO13', 'IGBP1', 'C1QTNF4', 'NUP62', 'BUB1', 'MRPL32', 'NUS1', 'MRPL42', 'N6AMT1', 'NARS', 'NSMCE2', 'TAF6', 'PPP4R2', 'POLR1E', 'TREML2', 'UBE2I', 'CBLL1', 'TSEN54', 'ELP2', 'COPS8', 'NUDT15', 'CHCHD1', 'NUP214', 'CALR', 'DCTN6', 'ATRIP', 'XRCC6', 'KDM2A', 'CWC15', 'BTAF1', 'ARMC7', 'WEE1', 'ALDOA', 'CUL1', 'NANOG', 'FASN', 'WDR74', 'BORA', 'SCAF1', 'UBR4', 'DNAJC17', 'DHX16', 'PELP1', 'YEATS2', 'PIK3C3', 'KCNA10', 'MCM3AP', 'NUP43', 'ADAT2', 'TAF6L', 'SP2', 'MRPS22', 'DHX36', 'MVD', 'UNCX', 'CHMP5', 'NMT1', 'UBA5', 'RINT1', 'OR2T29', 'PTCD1', 'ATL2', 'ETV4', 'OSGEP', 'SACM1L', 'RNMT', 'RPN2', 'GTF3C3', 'GRWD1', 'RPRD1B', 'RPA1', 'WDTC1', 'BAG6', 'HMGA1', 'ZC3H4', 'CDC5L', 'CCNH', 'UBE2Z', 'HIPK1', 'NBAS', 'RPN1', 'INTS2', 'TUBGCP3', 'INTS3', 'RSRC2', 'PSMD14', 'KATNB1', 'PSMB2', 'MRPS34', 'EIF2S3', 'CLNS1A', 'COX7B', 'SF3B4', 'PRXL2A', 'MBNL1', 'THOC3', 'MRPS31', 'RAN', 'RPL8', 'ATF5', 'GFER', 'ILF2', 'NOMO1', 'YBX3', 'RNF40', 'CEP152', 'PI4KA', 'TOE1', 'KAT7', 'CNOT11', 'PEX19', 'RPS27A', 'URM1', 'SYMPK', 'RMI1', 'STIL', 'SPC24', 'ACTR2', 'STRAP', 'DYNLL1', 'ZNF720', 'CHMP7', 'PSMB6', 'MOB4', 'TADA3', 'SYS1', 'PIK3R4', 'TRMT112', 'EP400', 'BET1', 'RPL35', 'RPS14', 'ZNF658', 'UQCRC2', 'TERF2', 'STX5', 'TTF2', 'HLA-C', 'SCFD1', 'GTF2H3', 'RRM1', 'HYPK', 'HAPLN2', 'ZNF84', 'MSL1', 'TMEM258', 'PIAS1', 'TPI1', 'TOMM20', 'CEP85', 'FRG2', 'CNOT2', 'GABPA', 'EIF2B4', 'PMPCA', 'LSM7', 'SNRPG', 'TAF2', 'HIST1H2BJ', 'HIST2H2BF', 'AP2S1', 'CSTF3', 'HCRTR1', 'UQCRQ', 'ZC3H18', 'GTF3C1', 'RPUSD4', 'NUTF2', 'MYB', 'EIF3J', 'COG4', 'COG1', 'SYF2', 'SNUPN', 'EIF4E', 'RPL24', 'RNPC3', 'TGS1', 'POP7', 'CHERP', 'NSF', 'ICE1', 'MRPS18C', 'CCT4', 'RTCB', 'ATP6V1D', 'VPS33A', 'XRCC3', 'FOXO1', 'EIF5A', 'SKA3', 'OR4N2', 'MRPL22', 'NUP35', 'HYOU1'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_means.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================GEARS DATA PREPARATION ===================\n",
    "\n",
    "# Import required libraries\n",
    "from gears import PertData, GEARS\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists('../../data/gears_predictions.pkl'):\n",
    "\n",
    "    print(\"Starting GEARS data preparation...\")\n",
    "\n",
    "    # Create a copy of the original data and subsample cells\n",
    "    print(\"Creating data copy...\")\n",
    "    adata_gears = adata.copy()\n",
    "    np.random.seed(42)  # Set random seed for reproducibility\n",
    "\n",
    "    adata_gears = adata_gears.copy()\n",
    "\n",
    "    # Process condition labels\n",
    "    print(\"Processing condition labels...\")\n",
    "    adata_gears.obs['condition'] = adata_gears.obs['condition'].astype(str) + '+ctrl'\n",
    "    adata_gears.obs['condition'] = adata_gears.obs['condition'].str.replace('control+ctrl', 'ctrl')\n",
    "\n",
    "    # Get unique perturbations\n",
    "    print(\"Getting unique perturbations...\")\n",
    "    all_perturbations = adata_gears.obs['condition'].unique()\n",
    "    all_perturbations = all_perturbations[all_perturbations != 'ctrl']\n",
    "    print(f\"Found {len(all_perturbations)} unique perturbations\")\n",
    "\n",
    "    # Split perturbations into first and second half\n",
    "    print(\"Splitting perturbations into halves...\")\n",
    "    first_half_perturbations = all_perturbations[:len(all_perturbations)//2]\n",
    "    second_half_perturbations = all_perturbations[len(all_perturbations)//2:]\n",
    "    print(f\"First half: {len(first_half_perturbations)} perturbations\")\n",
    "    print(f\"Second half: {len(second_half_perturbations)} perturbations\")\n",
    "\n",
    "    # Process first half splits\n",
    "    print(\"\\nProcessing first half splits...\")\n",
    "    first_half_train_val = first_half_perturbations\n",
    "    np.random.shuffle(first_half_train_val)\n",
    "    split_idx = int(len(first_half_train_val) * 0.9) # 90% train, 10% val\n",
    "    first_half_train = first_half_train_val[:split_idx]\n",
    "    first_half_val = first_half_train_val[split_idx:]\n",
    "\n",
    "    first_half_split_dict = {\n",
    "        'train': first_half_train,\n",
    "        'val': first_half_val,\n",
    "        'test': first_half_perturbations  # All first half perturbations as test\n",
    "    }\n",
    "    print(f\"First half splits - Train: {len(first_half_train)}, Val: {len(first_half_val)}\")\n",
    "\n",
    "    # Process second half splits\n",
    "    print(\"\\nProcessing second half splits...\")\n",
    "    second_half_train_val = second_half_perturbations\n",
    "    np.random.shuffle(second_half_train_val) \n",
    "    split_idx = int(len(second_half_train_val) * 0.9)  # 90% train, 10% val\n",
    "    second_half_train = second_half_train_val[:split_idx]\n",
    "    second_half_val = second_half_train_val[split_idx:]\n",
    "\n",
    "    second_half_split_dict = {\n",
    "        'train': second_half_train,\n",
    "        'val': second_half_val,\n",
    "        'test': second_half_perturbations  # All second half perturbations as test\n",
    "    }\n",
    "    print(f\"Second half splits - Train: {len(second_half_train)}, Val: {len(second_half_val)}\")\n",
    "\n",
    "    # Prepare data for GEARS\n",
    "    print(\"\\nPreparing data for GEARS...\")\n",
    "    # Add control to perturbation lists\n",
    "    first_half_perturbations = np.concatenate([['ctrl'], first_half_perturbations])\n",
    "    second_half_perturbations = np.concatenate([['ctrl'], second_half_perturbations])\n",
    "\n",
    "    # Create subsetted datasets\n",
    "    print(\"Creating subsetted datasets...\")\n",
    "    adata_first_half_gears = adata_gears[adata_gears.obs['condition'].isin(first_half_perturbations)].copy()\n",
    "    adata_second_half_gears = adata_gears[adata_gears.obs['condition'].isin(second_half_perturbations)].copy()\n",
    "    print(f\"First half dataset size: {adata_first_half_gears.n_obs} cells\")\n",
    "    print(f\"Second half dataset size: {adata_second_half_gears.n_obs} cells\")\n",
    "\n",
    "    # Process first half data\n",
    "    print(\"\\nProcessing first half data with GEARS...\")\n",
    "    pert_data_first_half = PertData('../../data')\n",
    "    pert_data_first_half.new_data_process(dataset_name='replogle22_gears_first_half_gears', adata=adata_first_half_gears)\n",
    "    # Filter out genes not in gene2go from each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        original_count = len(first_half_split_dict[split])\n",
    "        first_half_split_dict[split] = [gene for gene in first_half_split_dict[split] \n",
    "                                    if gene.replace('+ctrl', '') in pert_data_first_half.gene2go.keys()]\n",
    "        filtered_count = len(first_half_split_dict[split])\n",
    "        print(f\"{split} split: {filtered_count}/{original_count} genes kept ({original_count - filtered_count} removed)\")\n",
    "    with open('../../data/replogle22_gears_first_half_split_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(first_half_split_dict, f)\n",
    "    pert_data_first_half.load(data_path='../../data/replogle22_gears_first_half_gears')\n",
    "    pert_data_first_half.prepare_split(split='custom', seed=42, split_dict_path='../../data/replogle22_gears_first_half_split_dict.pkl')\n",
    "    print(\"First half data processing complete\")\n",
    "\n",
    "    # Process second half data\n",
    "    print(\"\\nProcessing second half data with GEARS...\")\n",
    "    pert_data_second_half = PertData('../../data')\n",
    "    pert_data_second_half.new_data_process(dataset_name='replogle22_gears_second_half_gears', adata=adata_second_half_gears)\n",
    "    # Filter out genes not in gene2go from each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        original_count = len(second_half_split_dict[split])\n",
    "        second_half_split_dict[split] = [gene for gene in second_half_split_dict[split] \n",
    "                                    if gene.replace('+ctrl', '') in pert_data_second_half.gene2go.keys()]\n",
    "        filtered_count = len(second_half_split_dict[split])\n",
    "        print(f\"{split} split: {filtered_count}/{original_count} genes kept ({original_count - filtered_count} removed)\")\n",
    "    with open('../../data/replogle22_gears_second_half_split_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(second_half_split_dict, f)\n",
    "    pert_data_second_half.load(data_path='../../data/replogle22_gears_second_half_gears')\n",
    "    pert_data_second_half.prepare_split(split='custom', seed=42, split_dict_path='../../data/replogle22_gears_second_half_split_dict.pkl')\n",
    "    print(\"Second half data processing complete\")\n",
    "\n",
    "    print(\"\\nGEARS data preparation completed successfully!\")\n",
    "\n",
    "\n",
    "    # ===================GEARS MODEL TRAINING ===================\n",
    "\n",
    "    pert_data_first_half.get_dataloader(batch_size = 32, test_batch_size = 512)\n",
    "    pert_data_second_half.get_dataloader(batch_size = 32, test_batch_size = 512)\n",
    "\n",
    "    gears_model_first_half = GEARS(pert_data_first_half, device = 'cuda', \n",
    "                            weight_bias_track = False, \n",
    "                            proj_name = 'first_half', \n",
    "                            exp_name = 'first_half')\n",
    "    gears_model_first_half.model_initialize()\n",
    "\n",
    "    gears_model_second_half = GEARS(pert_data_second_half, device = 'cuda', \n",
    "                            weight_bias_track = False, \n",
    "                            proj_name = 'second_half', \n",
    "                            exp_name = 'second_half')\n",
    "    gears_model_second_half.model_initialize()\n",
    "\n",
    "    gears_model_first_half.train(epochs = 10)\n",
    "    gears_model_second_half.train(epochs = 10)\n",
    "\n",
    "    os.makedirs(\"../../data/gears_models\", exist_ok=True)\n",
    "\n",
    "    gears_model_first_half.save_model('../../data/gears_models/first_half')\n",
    "    gears_model_second_half.save_model('../../data/gears_models/second_half')\n",
    "\n",
    "\n",
    "\n",
    "    # ===================GEARS PREDICTIONS ===================\n",
    "\n",
    "    # Compile GEARS predictions\n",
    "    first_half_perturbations = [gene for gene in first_half_perturbations if gene.replace('+ctrl', '') in pert_data_first_half.gene2go.keys()]\n",
    "    second_half_perturbations = [gene for gene in second_half_perturbations if gene.replace('+ctrl', '') in pert_data_second_half.gene2go.keys()]\n",
    "\n",
    "    first_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in first_half_perturbations if gene != 'ctrl']\n",
    "    second_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in second_half_perturbations if gene != 'ctrl']\n",
    "\n",
    "    predictions_first_half = gears_model_second_half.predict(first_half_perturbations_without_ctrl)\n",
    "    predictions_second_half = gears_model_first_half.predict(second_half_perturbations_without_ctrl)\n",
    "\n",
    "    # Combine predictions from both halves\n",
    "    gears_predictions = {}\n",
    "    for pert in tqdm(predictions_first_half.keys()):\n",
    "        gears_predictions[pert] = predictions_first_half[pert]\n",
    "    for pert in tqdm(predictions_second_half.keys()):\n",
    "        gears_predictions[pert] = predictions_second_half[pert]\n",
    "    gears_predictions['control'] = pert_means['control']\n",
    "\n",
    "    # Save GEARS predictions to pickle file\n",
    "    with open('../../data/gears_predictions.pkl', 'wb') as f:\n",
    "        pickle.dump(gears_predictions, f)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load GEARS predictions from pickle file\n",
    "    with open('../../data/gears_predictions.pkl', 'rb') as f:\n",
    "        gears_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scGPT data preparation...\n",
      "Creating data copy...\n",
      "Creating zero-filled AnnData object...\n",
      "Concatenating datasets...\n",
      "Final dataset size: 93568 cells, 8544 genes\n",
      "Processing condition labels...\n",
      "Getting unique perturbations...\n",
      "Found 1334 unique perturbations\n",
      "Splitting perturbations into halves...\n",
      "\n",
      "Processing first half splits...\n",
      "\n",
      "Preparing data for scGPT...\n",
      "First half: 659 perturbations\n",
      "Second half: 665 perturbations\n",
      "Creating subsetted datasets...\n",
      "First half dataset size: 50304 cells\n",
      "Second half dataset size: 50688 cells\n",
      "\n",
      "Processing first half data with scGPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:66\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First half data processing complete\n",
      "\n",
      "Processing second half data with scGPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:0\n",
      "combo_seen1:0\n",
      "combo_seen2:0\n",
      "unseen_single:67\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second half data processing complete\n",
      "\\scGPT data preparation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ===================SCGPT DATA PREPARATION ===================\n",
    "\n",
    "# Import required libraries\n",
    "from gears import PertData, GEARS\n",
    "import pickle\n",
    "\n",
    "print(\"Starting scGPT data preparation...\")\n",
    "\n",
    "# Create a copy of the original data and subsample cells\n",
    "print(\"Creating data copy...\")\n",
    "adata_gears = adata.copy()\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "\n",
    "# Create a new AnnData object with zeros for all genes\n",
    "print(\"Creating zero-filled AnnData object...\")\n",
    "perturbed_genes = list(adata_gears.obs['condition'].unique())\n",
    "perturbed_genes.remove('control')\n",
    "missing_perturbed_genes = [gene for gene in perturbed_genes if gene not in adata_gears.var.gene_name.tolist()]\n",
    "zero_adata = sc.AnnData(\n",
    "    X=np.zeros((adata_gears.n_obs, len(missing_perturbed_genes))),\n",
    "    obs=adata_gears.obs.copy(),\n",
    "    var=pd.DataFrame(index=missing_perturbed_genes)\n",
    ")\n",
    "zero_adata.var['gene_name'] = missing_perturbed_genes\n",
    "\n",
    "# Concatenate the original and zero-filled data along the genes axis\n",
    "print(\"Concatenating datasets...\")\n",
    "adata_gears = sc.concat([adata_gears, zero_adata], axis=1, join='outer', merge='first')\n",
    "\n",
    "print(f\"Final dataset size: {adata_gears.n_obs} cells, {adata_gears.n_vars} genes\")\n",
    "\n",
    "# Process condition labels\n",
    "print(\"Processing condition labels...\")\n",
    "adata_gears.obs['condition'] = adata_gears.obs['condition'].astype(str) + '+ctrl'\n",
    "adata_gears.obs['condition'] = adata_gears.obs['condition'].str.replace('control+ctrl', 'ctrl')\n",
    "\n",
    "# Get unique perturbations\n",
    "print(\"Getting unique perturbations...\")\n",
    "all_perturbations = adata_gears.obs['condition'].unique()\n",
    "all_perturbations = all_perturbations[all_perturbations != 'ctrl']\n",
    "print(f\"Found {len(all_perturbations)} unique perturbations\")\n",
    "\n",
    "# Split perturbations into first and second half\n",
    "print(\"Splitting perturbations into halves...\")\n",
    "first_half_perturbations = all_perturbations[:len(all_perturbations)//2]\n",
    "second_half_perturbations = all_perturbations[len(all_perturbations)//2:]\n",
    "\n",
    "# Process first half splits\n",
    "print(\"\\nProcessing first half splits...\")\n",
    "first_half_train_val = first_half_perturbations\n",
    "np.random.shuffle(first_half_train_val)\n",
    "split_idx = int(len(first_half_train_val) * 0.9) # 90% train, 10% val\n",
    "first_half_train = first_half_train_val[:split_idx]\n",
    "first_half_val = first_half_train_val[split_idx:]\n",
    "\n",
    "# Prepare data for scGPT\n",
    "print(\"\\nPreparing data for scGPT...\")\n",
    "# Load gene2go mapping file\n",
    "gene2go_path = \"../../data/gene2go_all.pkl\"\n",
    "with open(gene2go_path, 'rb') as f:\n",
    "    gene2go = pickle.load(f)\n",
    "first_half_perturbations = [pert for pert in first_half_perturbations if pert.replace('+ctrl','') in list(gene2go.keys())]\n",
    "second_half_perturbations = [pert for pert in second_half_perturbations if pert.replace('+ctrl','') in list(gene2go.keys())]\n",
    "# Add control to perturbation lists\n",
    "first_half_perturbations = np.concatenate([['ctrl'], first_half_perturbations])\n",
    "second_half_perturbations = np.concatenate([['ctrl'], second_half_perturbations])\n",
    "\n",
    "print(f\"First half: {len(first_half_perturbations)} perturbations\")\n",
    "print(f\"Second half: {len(second_half_perturbations)} perturbations\")\n",
    "\n",
    "# Create subsetted datasets\n",
    "print(\"Creating subsetted datasets...\")\n",
    "adata_first_half_gears = adata_gears[adata_gears.obs['condition'].isin(first_half_perturbations)].copy()\n",
    "adata_second_half_gears = adata_gears[adata_gears.obs['condition'].isin(second_half_perturbations)].copy()\n",
    "print(f\"First half dataset size: {adata_first_half_gears.n_obs} cells\")\n",
    "print(f\"Second half dataset size: {adata_second_half_gears.n_obs} cells\")\n",
    "\n",
    "# Process first half data\n",
    "print(\"\\nProcessing first half data with scGPT...\")\n",
    "pert_data_first_half = PertData('../../data')\n",
    "pert_data_first_half.new_data_process(dataset_name='replogle22_gears_first_half_scgpt', adata=adata_first_half_gears)\n",
    "pert_data_first_half.load(data_path='../../data/replogle22_gears_first_half_scgpt')\n",
    "pert_data_first_half.prepare_split(split='simulation', seed=42, train_gene_set_size = 0.9)\n",
    "print(\"First half data processing complete\")\n",
    "\n",
    "# Process second half data\n",
    "print(\"\\nProcessing second half data with scGPT...\")\n",
    "pert_data_second_half = PertData('../../data')\n",
    "pert_data_second_half.new_data_process(dataset_name='replogle22_gears_second_half_scgpt', adata=adata_second_half_gears)\n",
    "pert_data_second_half.load(data_path='../../data/replogle22_gears_second_half_scgpt')\n",
    "pert_data_second_half.prepare_split(split='simulation', seed=42, train_gene_set_size = 0.9)\n",
    "print(\"Second half data processing complete\")\n",
    "\n",
    "pert_data_first_half.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "pert_data_second_half.get_dataloader(batch_size=64, test_batch_size=64)\n",
    "\n",
    "print(\"\\scGPT data preparation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to ../../data/save/dev_perturb_reploggle22-May23-09-55\n",
      "scGPT - INFO - Running on 2025-05-23 09:55:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 8387/8544 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../../data/scGPT_CP/best_model.pt, the model args will override the config ../../data/scGPT_CP/args.json.\n",
      "scGPT - INFO - Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading parameter encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.3.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.3.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.5.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter cls_decoder._decoder.5.bias with shape torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# ===================SCGPT DEPENDENCIES LOAD ===================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gears import PertData, GEARS\n",
    "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_relative_error,\n",
    ")\n",
    "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import set_seed, map_raw_id_to_vocab_id, compute_perturbation_metrics\n",
    "\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scgpt.utils import load_pretrained\n",
    "\n",
    "import gdown\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "model_dir = \"../../data/scGPT_CP\"\n",
    "if not os.path.exists(model_dir):\n",
    "    !mkdir -p $model_dir\n",
    "    gdown.download_folder(\n",
    "        \"https://drive.google.com/drive/folders/1_GROJTzXiAV8HB4imruOTk6PEGuNOcgB?usp=sharing\",\n",
    "        output=model_dir,\n",
    "    )\n",
    "\n",
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 0\n",
    "include_zero_gene = \"all\"\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "amp = True\n",
    "load_model = \"../../data/scGPT_CP\"\n",
    "load_param_prefixs = [\n",
    "    \"encoder\",\n",
    "    \"value_encoder\",\n",
    "    \"transformer_encoder\",\n",
    "]\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4  # or 1e-4\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "epochs = 10\n",
    "schedule_interval = 1\n",
    "early_stop = 5\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "n_layers_cls = 3\n",
    "dropout = 0  # dropout probability\n",
    "use_fast_transformer = True  # whether to use fast transformer\n",
    "\n",
    "# logging\n",
    "log_interval = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_name = 'reploggle22'\n",
    "\n",
    "save_dir = Path(f\"../../data/save/dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"saving to {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "# log running date and current git commit\n",
    "logger.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "model_dir = Path(load_model)\n",
    "model_config_file = model_dir / \"args.json\"\n",
    "model_file = model_dir / \"best_model.pt\"\n",
    "vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "vocab = GeneVocab.from_file(vocab_file)\n",
    "for s in special_tokens:\n",
    "    if s not in vocab:\n",
    "        vocab.append_token(s)\n",
    "\n",
    "pert_data_first_half.adata.var[\"id_in_vocab\"] = [\n",
    "    1 if gene in vocab else -1 for gene in pert_data_first_half.adata.var[\"gene_name\"]\n",
    "]\n",
    "gene_ids_in_vocab = np.array(pert_data_first_half.adata.var[\"id_in_vocab\"])\n",
    "logger.info(\n",
    "    f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "    f\"in vocabulary of size {len(vocab)}.\"\n",
    ")\n",
    "genes = pert_data_first_half.adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "# model\n",
    "with open(model_config_file, \"r\") as f:\n",
    "    model_configs = json.load(f)\n",
    "logger.info(\n",
    "    f\"Resume model from {model_file}, the model args will override the \"\n",
    "    f\"config {model_config_file}.\"\n",
    ")\n",
    "embsize = model_configs[\"embsize\"]\n",
    "nhead = model_configs[\"nheads\"]\n",
    "d_hid = model_configs[\"d_hid\"]\n",
    "nlayers = model_configs[\"nlayers\"]\n",
    "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)\n",
    "\n",
    "pretrained_dict = torch.load(model_file)\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model_first_half = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "model_first_half = load_pretrained(model_first_half, pretrained_dict)\n",
    "model_first_half = model_first_half.to(device)\n",
    "\n",
    "# Create a copy for second half\n",
    "model_second_half = TransformerGenerator(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    pert_pad_id=pert_pad_id,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "model_second_half.load_state_dict(model_first_half.state_dict())\n",
    "model_second_half = model_second_half.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - | epoch   1 | 100/660 batches | lr 0.0001 | ms/batch 1082.98 | loss  0.15 | mse  0.15 |\n",
      "scGPT - INFO - | epoch   1 | 200/660 batches | lr 0.0001 | ms/batch 1066.95 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 300/660 batches | lr 0.0001 | ms/batch 1067.30 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 400/660 batches | lr 0.0001 | ms/batch 1067.41 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 500/660 batches | lr 0.0001 | ms/batch 1067.40 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   1 | 600/660 batches | lr 0.0001 | ms/batch 1066.85 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - First half val_metrics at epoch 1: \n",
      "scGPT - INFO - {'pearson': 0.9877293925702616, 'pearson_de': 0.9439734139121962, 'pearson_delta': 0.1510679987018839, 'pearson_de_delta': 0.44131858406826324}\n",
      "scGPT - INFO - | end of epoch   1 | time: 927.12s | \n",
      "scGPT - INFO - Best first half model with score 0.9877\n",
      "scGPT - INFO - | epoch   2 | 100/660 batches | lr 0.0001 | ms/batch 1079.16 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   2 | 200/660 batches | lr 0.0001 | ms/batch 1067.85 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   2 | 300/660 batches | lr 0.0001 | ms/batch 1067.83 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   2 | 400/660 batches | lr 0.0001 | ms/batch 1067.60 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   2 | 500/660 batches | lr 0.0001 | ms/batch 1067.55 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   2 | 600/660 batches | lr 0.0001 | ms/batch 1067.49 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - First half val_metrics at epoch 2: \n",
      "scGPT - INFO - {'pearson': 0.9858639886505169, 'pearson_de': 0.9445670504493165, 'pearson_delta': 0.14525058798896492, 'pearson_de_delta': 0.3754357304703281}\n",
      "scGPT - INFO - | end of epoch   2 | time: 926.00s | \n",
      "scGPT - INFO - | epoch   3 | 100/660 batches | lr 0.0001 | ms/batch 1078.36 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   3 | 200/660 batches | lr 0.0001 | ms/batch 1067.18 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   3 | 300/660 batches | lr 0.0001 | ms/batch 1067.09 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   3 | 400/660 batches | lr 0.0001 | ms/batch 1067.01 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   3 | 500/660 batches | lr 0.0001 | ms/batch 1066.97 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   3 | 600/660 batches | lr 0.0001 | ms/batch 1067.09 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - First half val_metrics at epoch 3: \n",
      "scGPT - INFO - {'pearson': 0.9875514774279525, 'pearson_de': 0.940430234159218, 'pearson_delta': 0.1684167892708198, 'pearson_de_delta': 0.3879200401753359}\n",
      "scGPT - INFO - | end of epoch   3 | time: 925.34s | \n",
      "scGPT - INFO - Early stop at epoch 3\n",
      "scGPT - INFO - | epoch   1 | 100/665 batches | lr 0.0001 | ms/batch 1080.93 | loss  0.15 | mse  0.15 |\n",
      "scGPT - INFO - | epoch   1 | 200/665 batches | lr 0.0001 | ms/batch 1068.62 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 300/665 batches | lr 0.0001 | ms/batch 1067.87 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 400/665 batches | lr 0.0001 | ms/batch 1067.00 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 500/665 batches | lr 0.0001 | ms/batch 1067.07 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   1 | 600/665 batches | lr 0.0001 | ms/batch 1067.15 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - Second half val_metrics at epoch 1: \n",
      "scGPT - INFO - {'pearson': 0.9825439685322847, 'pearson_de': 0.9499926955249217, 'pearson_delta': 0.29733688158274224, 'pearson_de_delta': 0.49886399731957315}\n",
      "scGPT - INFO - | end of epoch   1 | time: 932.41s | \n",
      "scGPT - INFO - Best second half model with score 0.9825\n",
      "scGPT - INFO - | epoch   2 | 100/665 batches | lr 0.0001 | ms/batch 1079.08 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   2 | 200/665 batches | lr 0.0001 | ms/batch 1067.76 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   2 | 300/665 batches | lr 0.0001 | ms/batch 1067.75 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   2 | 400/665 batches | lr 0.0001 | ms/batch 1067.82 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   2 | 500/665 batches | lr 0.0001 | ms/batch 1067.54 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   2 | 600/665 batches | lr 0.0001 | ms/batch 1067.60 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - Second half val_metrics at epoch 2: \n",
      "scGPT - INFO - {'pearson': 0.9827207578351053, 'pearson_de': 0.9474438095948017, 'pearson_delta': 0.2738244936760848, 'pearson_de_delta': 0.394176130037767}\n",
      "scGPT - INFO - | end of epoch   2 | time: 931.29s | \n",
      "scGPT - INFO - Best second half model with score 0.9827\n",
      "scGPT - INFO - | epoch   3 | 100/665 batches | lr 0.0001 | ms/batch 1078.10 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   3 | 200/665 batches | lr 0.0001 | ms/batch 1067.15 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   3 | 300/665 batches | lr 0.0001 | ms/batch 1067.08 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   3 | 400/665 batches | lr 0.0001 | ms/batch 1066.97 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   3 | 500/665 batches | lr 0.0001 | ms/batch 1067.01 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   3 | 600/665 batches | lr 0.0001 | ms/batch 1066.89 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - Second half val_metrics at epoch 3: \n",
      "scGPT - INFO - {'pearson': 0.9812593659569273, 'pearson_de': 0.9412375941397955, 'pearson_delta': 0.2805268396955058, 'pearson_de_delta': 0.4937118725265829}\n",
      "scGPT - INFO - | end of epoch   3 | time: 930.31s | \n",
      "scGPT - INFO - | epoch   4 | 100/665 batches | lr 0.0001 | ms/batch 1077.82 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   4 | 200/665 batches | lr 0.0001 | ms/batch 1067.03 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   4 | 300/665 batches | lr 0.0001 | ms/batch 1067.10 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   4 | 400/665 batches | lr 0.0001 | ms/batch 1066.95 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   4 | 500/665 batches | lr 0.0001 | ms/batch 1066.94 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   4 | 600/665 batches | lr 0.0001 | ms/batch 1066.89 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - Second half val_metrics at epoch 4: \n",
      "scGPT - INFO - {'pearson': 0.9827759065770679, 'pearson_de': 0.9439291639917732, 'pearson_delta': 0.30549255881612336, 'pearson_de_delta': 0.5169137700093933}\n",
      "scGPT - INFO - | end of epoch   4 | time: 930.05s | \n",
      "scGPT - INFO - Best second half model with score 0.9828\n",
      "scGPT - INFO - | epoch   5 | 100/665 batches | lr 0.0001 | ms/batch 1077.28 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   5 | 200/665 batches | lr 0.0001 | ms/batch 1066.49 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   5 | 300/665 batches | lr 0.0001 | ms/batch 1066.42 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   5 | 400/665 batches | lr 0.0001 | ms/batch 1066.34 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   5 | 500/665 batches | lr 0.0001 | ms/batch 1066.35 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   5 | 600/665 batches | lr 0.0001 | ms/batch 1066.31 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - Second half val_metrics at epoch 5: \n",
      "scGPT - INFO - {'pearson': 0.9825780803699475, 'pearson_de': 0.9401830917623982, 'pearson_delta': 0.32006265740134854, 'pearson_de_delta': 0.4888655270656406}\n",
      "scGPT - INFO - | end of epoch   5 | time: 928.93s | \n",
      "scGPT - INFO - | epoch   6 | 100/665 batches | lr 0.0001 | ms/batch 1077.31 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   6 | 200/665 batches | lr 0.0001 | ms/batch 1066.42 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   6 | 300/665 batches | lr 0.0001 | ms/batch 1066.48 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - | epoch   6 | 400/665 batches | lr 0.0001 | ms/batch 1066.63 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   6 | 500/665 batches | lr 0.0001 | ms/batch 1066.54 | loss  0.12 | mse  0.12 |\n",
      "scGPT - INFO - | epoch   6 | 600/665 batches | lr 0.0001 | ms/batch 1066.50 | loss  0.13 | mse  0.13 |\n",
      "scGPT - INFO - Second half val_metrics at epoch 6: \n",
      "scGPT - INFO - {'pearson': 0.9826952928421486, 'pearson_de': 0.9460432839791906, 'pearson_delta': 0.33713837380201256, 'pearson_de_delta': 0.5698665213026671}\n",
      "scGPT - INFO - | end of epoch   6 | time: 929.14s | \n",
      "scGPT - INFO - Early stop at epoch 6\n"
     ]
    }
   ],
   "source": [
    "# ===================SCGPT FINETUNING ===================\n",
    "\n",
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "# First half model optimizers and schedulers\n",
    "optimizer_first = torch.optim.Adam(model_first_half.parameters(), lr=lr)\n",
    "scheduler_first = torch.optim.lr_scheduler.StepLR(optimizer_first, schedule_interval, gamma=0.9)\n",
    "scaler_first = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "# Second half model optimizers and schedulers  \n",
    "optimizer_second = torch.optim.Adam(model_second_half.parameters(), lr=lr)\n",
    "scheduler_second = torch.optim.lr_scheduler.StepLR(optimizer_second, schedule_interval, gamma=0.9)\n",
    "scaler_second = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "\n",
    "# First half model training\n",
    "best_val_loss_first = float(\"inf\")\n",
    "best_val_corr_first = 0\n",
    "best_model_first = None\n",
    "patience_first = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loader = pert_data_first_half.dataloader[\"train_loader\"]\n",
    "    valid_loader = pert_data_first_half.dataloader[\"val_loader\"]\n",
    "\n",
    "    train(\n",
    "        model_first_half,\n",
    "        train_loader,\n",
    "        epoch,\n",
    "        device,\n",
    "        criterion,\n",
    "        optimizer_first,\n",
    "        scheduler_first,\n",
    "        scaler_first,\n",
    "        n_genes,\n",
    "        gene_ids,\n",
    "        include_zero_gene,\n",
    "        max_seq_len,\n",
    "        CLS,\n",
    "        CCE,\n",
    "        MVC,\n",
    "        ECS,\n",
    "        log_interval,\n",
    "        logger\n",
    "    )\n",
    "\n",
    "    val_res = eval_perturb(valid_loader, model_first_half, device, include_zero_gene, gene_ids)\n",
    "    val_metrics = compute_perturbation_metrics(\n",
    "        val_res, pert_data_first_half.adata[pert_data_first_half.adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    )\n",
    "    logger.info(f\"First half val_metrics at epoch {epoch}: \")\n",
    "    logger.info(val_metrics)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \")\n",
    "\n",
    "    val_score = val_metrics[\"pearson_de_delta\"]\n",
    "    if val_score > best_val_corr_first:\n",
    "        best_val_corr_first = val_score\n",
    "        best_model_first_half = copy.deepcopy(model_first_half)\n",
    "        logger.info(f\"Best first half model with score {val_score:5.4f}\")\n",
    "        patience_first = 0\n",
    "    else:\n",
    "        patience_first += 1\n",
    "        if patience_first >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    scheduler_first.step()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Second half model training\n",
    "best_val_loss_second = float(\"inf\")\n",
    "best_val_corr_second = 0\n",
    "best_model_second = None\n",
    "patience_second = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loader = pert_data_second_half.dataloader[\"train_loader\"]\n",
    "    valid_loader = pert_data_second_half.dataloader[\"val_loader\"]\n",
    "\n",
    "    train(\n",
    "        model_second_half,\n",
    "        train_loader,\n",
    "        epoch,\n",
    "        device,\n",
    "        criterion,\n",
    "        optimizer_second,\n",
    "        scheduler_second,\n",
    "        scaler_second,\n",
    "        n_genes,\n",
    "        gene_ids,\n",
    "        include_zero_gene,\n",
    "        max_seq_len,\n",
    "        CLS,\n",
    "        CCE,\n",
    "        MVC,\n",
    "        ECS,\n",
    "        log_interval,\n",
    "        logger\n",
    "    )\n",
    "\n",
    "    val_res = eval_perturb(valid_loader, model_second_half, device, include_zero_gene, gene_ids)\n",
    "    val_metrics = compute_perturbation_metrics(\n",
    "        val_res, pert_data_second_half.adata[pert_data_second_half.adata.obs[\"condition\"] == \"ctrl\"]\n",
    "    )\n",
    "    logger.info(f\"Second half val_metrics at epoch {epoch}: \")\n",
    "    logger.info(val_metrics)\n",
    "\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \")\n",
    "\n",
    "    val_score = val_metrics[\"pearson_de_delta\"]\n",
    "    if val_score > best_val_corr_second:\n",
    "        best_val_corr_second = val_score\n",
    "        best_model_second_half = copy.deepcopy(model_second_half)\n",
    "        logger.info(f\"Best second half model with score {val_score:5.4f}\")\n",
    "        patience_second = 0\n",
    "    else:\n",
    "        patience_second += 1\n",
    "        if patience_second >= early_stop:\n",
    "            logger.info(f\"Early stop at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    scheduler_second.step()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/658 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 658/658 [40:18<00:00,  3.67s/it]\n",
      "100%|██████████| 664/664 [40:58<00:00,  3.70s/it]\n",
      "100%|██████████| 658/658 [00:00<00:00, 2926.61it/s]\n",
      "100%|██████████| 664/664 [00:00<00:00, 2972.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# ===================SCGPT PREDICTIONS ===================\n",
    "\n",
    "scgpt_gene_indices = [adata.var.index.get_loc(gene) for gene in adata.var.index if gene in adata_gears.var.index]\n",
    "\n",
    "first_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in first_half_perturbations if gene != 'ctrl']\n",
    "second_half_perturbations_without_ctrl = [[gene.replace('+ctrl', '')] for gene in second_half_perturbations if gene != 'ctrl']\n",
    "\n",
    "predictions_first_half = predict(\n",
    "    best_model_second_half,\n",
    "    first_half_perturbations_without_ctrl,\n",
    "    pert_data_first_half,\n",
    "    gene_ids=gene_ids,\n",
    "    include_zero_gene=include_zero_gene,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    amp=amp, pool_size=64\n",
    ")\n",
    "predictions_second_half = predict(\n",
    "    best_model_first_half,\n",
    "    second_half_perturbations_without_ctrl,\n",
    "    pert_data_second_half,\n",
    "    gene_ids=gene_ids,\n",
    "    include_zero_gene=include_zero_gene,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    amp=amp, pool_size=64\n",
    ")\n",
    "\n",
    "# Combine predictions from both halves\n",
    "scgpt_predictions = {}\n",
    "for pert in tqdm(predictions_first_half.keys()):\n",
    "    scgpt_predictions[pert] = predictions_first_half[pert][scgpt_gene_indices]\n",
    "for pert in tqdm(predictions_second_half.keys()):\n",
    "    scgpt_predictions[pert] = predictions_second_half[pert][scgpt_gene_indices]\n",
    "scgpt_predictions['control'] = pert_means['control']\n",
    "\n",
    "# Save scGPT predictions to pickle file\n",
    "with open('../../data/scgpt_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(scgpt_predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
